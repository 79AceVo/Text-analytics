{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "Two of the ways to further reduce the number of words but still keep the roots of words are via stemming and lemmatization. Stemming is to cutting off the word while lemmatization is to finding the root word\n",
    "\n",
    "Example for stemming:\n",
    "\n",
    "* adjustable --> adjust\n",
    "\n",
    "* formality --> formaliti (a stemmed token does not need to be a real word)\n",
    "\n",
    "\n",
    "Example for lemmatization:\n",
    "\n",
    "* am --> be\n",
    "\n",
    "* better --> good (a lemma token is a real word)\n",
    "\n",
    "To create stemming and lemmatization of the word, we lean on NLTK package for this exercise. There are many NLP packages, NLTK is one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "NLTK has stemmers.\n",
    "\n",
    "TextBlob has stemmers too.\n",
    "\n",
    "Spacy has none.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = ['run',\n",
    "                 'runner',\n",
    "                 'running',\n",
    "                 'ran',\n",
    "                 'runs',\n",
    "                 'easily',\n",
    "                 'fairly',\n",
    "                 \"cook\",\n",
    "                 \"cooker\",\n",
    "                 \"cooking\",\n",
    "                 \"cooked\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'runner',\n",
       " 'run',\n",
       " 'ran',\n",
       " 'run',\n",
       " 'easili',\n",
       " 'fairli',\n",
       " 'cook',\n",
       " 'cooker',\n",
       " 'cook',\n",
       " 'cook']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import * #import Stemming from NLTK\n",
    "\n",
    "# Initialize the stemmer\n",
    "porter = PorterStemmer() #porter is a popular one, there are many more https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "stem_example = [porter.stem(word) for word in list_of_words]\n",
    "stem_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'runner',\n",
       " 'run',\n",
       " 'ran',\n",
       " 'run',\n",
       " 'easili',\n",
       " 'fairli',\n",
       " 'cook',\n",
       " 'cooker',\n",
       " 'cook',\n",
       " 'cook']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "stemmed_words = [Word(word).stem() for word in list_of_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\avo9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\avo9\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'runner',\n",
       " 'running',\n",
       " 'ran',\n",
       " 'run',\n",
       " 'easily',\n",
       " 'fairly',\n",
       " 'cook',\n",
       " 'cooker',\n",
       " 'cooking',\n",
       " 'cooked']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4') #download omw-1.4 as a prereq for wordnet\n",
    "nltk.download('wordnet') #download Wordnet data for the lemmatization\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "nltk_example = [nltk_lemmatizer.lemmatize(word) for word in list_of_words]\n",
    "nltk_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'runner',\n",
       " 'run',\n",
       " 'run',\n",
       " 'run',\n",
       " 'easily',\n",
       " 'fairly',\n",
       " 'cook',\n",
       " 'cooker',\n",
       " 'cook',\n",
       " 'cook']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Spacy\n",
    "\n",
    "from spacy.cli import download\n",
    "download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatized_words = [nlp(word)[0].lemma_ for word in list_of_words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\avo9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'runner',\n",
       " 'running',\n",
       " 'ran',\n",
       " 'run',\n",
       " 'easily',\n",
       " 'fairly',\n",
       " 'cook',\n",
       " 'cooker',\n",
       " 'cooking',\n",
       " 'cooked']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Textblob\n",
    "#from textblob import Word\n",
    "nltk.download('wordnet') #lemma from Textblob uses Wordnet\n",
    "lemmatized_words = [Word(word).lemmatize() for word in list_of_words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOUZjpTO-KRH"
   },
   "source": [
    "# Tokenizing Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dcmc4P7hea8"
   },
   "source": [
    "# Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5RIR-V7HhrG3"
   },
   "outputs": [],
   "source": [
    "text = \"Dr. Smith’s AI-powered model (trained on Türkçe, 10+ languages) outperformed others in the '2023_Challenge'! #NLP.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XplXMBU_xJkZ"
   },
   "source": [
    "## Sentence Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vnalNypexMf8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\avo9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Sentences: [\"Dr. Smith’s AI-powered model (trained on Türkçe, 10+ languages) outperformed others in the '2023_Challenge'!\", '#NLP.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\") #download a pretrained tokenizer model\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk_sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"NLTK Sentences:\", nltk_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tcLQ3U-DxOyc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Sentences: [\"Dr. Smith’s AI-powered model (trained on Türkçe, 10+ languages) outperformed others in the '2023_Challenge'!\", '#NLP.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "spacy_sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "print(\"spaCy Sentences:\", spacy_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgmmpVApxlbv"
   },
   "source": [
    "## White Space tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YF-X1Lwcxdl0",
    "outputId": "ddcfc3ed-6d5a-4462-80c7-e7edc7f7875f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Dr.', 'Smith’s', 'AI-powered', 'model', '(trained', 'on', 'Türkçe,', '10+', 'languages)', 'outperformed', 'others', 'in', 'the', \"'2023_Challenge'!\", '#NLP.']\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "\n",
    "# Print tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQRBsyoMmDfU"
   },
   "source": [
    "## Word Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8EBw4qEkhhyc",
    "outputId": "f26bd38a-18c0-482b-d1d4-41c96ead0bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Word Tokens: ['Dr.', 'Smith', '’', 's', 'AI-powered', 'model', '(', 'trained', 'on', 'Türkçe', ',', '10+', 'languages', ')', 'outperformed', 'others', 'in', 'the', \"'2023_Challenge\", \"'\", '!', '#', 'NLP', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\avo9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\") #download a pretrained tokenizer model\n",
    "from nltk.tokenize import word_tokenize\n",
    "# NLTK tokenization\n",
    "\n",
    "nltk_word_tokens = word_tokenize(text)\n",
    "\n",
    "\n",
    "print(\"NLTK Word Tokens:\", nltk_word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8My915Rxhi3x",
    "outputId": "b1554dd9-7b75-4aaf-83cb-cebd711d0220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Word Tokens: ['Dr.', 'Smith', '’s', 'AI', '-', 'powered', 'model', '(', 'trained', 'on', 'Türkçe', ',', '10', '+', 'languages', ')', 'outperformed', 'others', 'in', 'the', \"'\", '2023_Challenge', \"'\", '!', '#', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"spaCy Word Tokens:\", spacy_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XepqpRHyA_oz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Dr', 'Smith', '’', 's', 'AI-powered', 'model', 'trained', 'on', 'Türkçe', '10', 'languages', 'outperformed', 'others', 'in', 'the', \"'2023_Challenge\", 'NLP'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob\n",
    "TextBlob(text).words #split to words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuHDzBt4yi67"
   },
   "source": [
    "##Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "c351d2ca3983464f9d9db492dece4994",
      "fcadb9bd034f495eae641e402371dd0c",
      "e737656199394ec4b4059626f9356252",
      "4d4e0d94b5e5409895685f0fb6e8f97b",
      "26abbe21d7234756a4620fd4e45796a4",
      "9f7b439f40bb4b6cb1d535ab489efea7",
      "b4dd7b19ebcf4999973ba6c272f5bdcc",
      "240af11476184803b336251622d2ff1a",
      "7927a603c1374371af0592e7b2fc08da",
      "968d77fd52344c66a0d0838a87c6da86",
      "05de38f8d725419f9e53f523d908be39",
      "ba6ca4691f78468c96a4ef08e2e5fe48",
      "b46c4f7e9ccc4caaa614cb337795603a",
      "b202bef6b4774e3da98efc2dd1c6608a",
      "66f8fcd023864b1b982f21e4f904ae84",
      "d9f640e17e9c400ca2cb86a4d89baaf8",
      "b44c07f3404a47b8a601fba48711042d",
      "2e00d59a929642d182fb0c08b2899116",
      "9ec632640af34c5c83cd1b91dca9671f",
      "1cb5cd0aeb5043138b86871bb05c4d55",
      "1e7c6d731644438490f361337c2c2762",
      "7cdfb379566149a0adea0fd2485aab56",
      "fdc353b13e5b4624bfad18ded31b9785",
      "971c81cb77d14969bfbc66a87c27a25e",
      "6cca8b3b2b4d49538cf2e1185e74c7ae",
      "a6b0e6249dba4b6496d587efd29b6c37",
      "4c5aea6d26114535b47b4f3ac8c48fe9",
      "621e49e4759047b4b6e18d948e2145aa",
      "e07545840c1b4f61b0fe74b34efdd361",
      "72ab21e596a74f30ba3f0d354d0dce30",
      "fa4353a138f0428bbb52a4b0909ff316",
      "8532ce3b52d941c1b0f2f592ba68224a",
      "29c34f6c709b4a77a54d02ee21b6e23c",
      "a2fcdf47203748ddb846a7321a20d6ae",
      "985a8463d73b45f7adda02816ee6e0b5",
      "52f046704af04d81be6d89d9f235e295",
      "5b54080e39c241daa6534016693927f5",
      "2b12367ea033413d9d96d46c14717856",
      "b7c43f67d1b4400f910a584b449567a7",
      "e6de953298f14f678f7b037f20751692",
      "fa30bc65e6824fafadbcad40278e819f",
      "c918cc5fc040456096dc25da92c26f55",
      "7c12b2ebc70e4cad8a01131e62246cb6",
      "a7e47c98eb8541039f3d87c39e706dce"
     ]
    },
    "id": "JebbLrv4k73z",
    "outputId": "4dc7e417-cd46-48bb-d989-58538bf97d3d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642140619de34034ba77a58946c04441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avo9\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\avo9\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f57f892e7c4142b4d47abe279aaf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59358e0738a45fda1769b2626935026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27da2cbd46eb40269a7588842e70b42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['dr', '.', 'smith', '’', 's', 'ai', '-', 'powered', 'model', '(', 'trained', 'on', 'turk', '##ce', ',', '10', '+', 'languages', ')', 'out', '##per', '##formed', 'others', 'in', 'the', \"'\", '202', '##3', '_', 'challenge', \"'\", '!', '#', 'nl', '##p', '.']\n",
      "Token IDs: [2852, 1012, 3044, 1521, 1055, 9932, 1011, 6113, 2944, 1006, 4738, 2006, 22883, 3401, 1010, 2184, 1009, 4155, 1007, 2041, 4842, 29021, 2500, 1999, 1996, 1005, 16798, 2509, 1035, 4119, 1005, 999, 1001, 17953, 2361, 1012]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Dr. Smith’s AI-powered model (trained on Türkçe, 10+ languages) outperformed others in the '2023_Challenge'! #NLP.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print tokens and IDs\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kj0l_EbgoDHz",
    "outputId": "3cd13101-1450-4190-82fb-e78d431809ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['dr', '.', 'smith', '’', 's', 'ai', '-', 'powered', 'model', '(', 'trained', 'on', 'turk', '##ce', ',', '10', '+', 'languages', ')', 'out', '##per', '##formed', 'others', 'in', 'the', \"'\", '202', '##3', '_', 'challenge', \"'\", '!', '#', 'nl', '##p', '.']\n",
      "Token IDs: [2852, 1012, 3044, 1521, 1055, 9932, 1011, 6113, 2944, 1006, 4738, 2006, 22883, 3401, 1010, 2184, 1009, 4155, 1007, 2041, 4842, 29021, 2500, 1999, 1996, 1005, 16798, 2509, 1035, 4119, 1005, 999, 1001, 17953, 2361, 1012]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Dr. Smith’s AI-powered model (trained on Türkçe, 10+ languages) outperformed others in the '2023_Challenge'! #NLP.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer2.tokenize(text)\n",
    "token_ids = tokenizer2.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print tokens and IDs\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_viTsfyt4I9",
    "outputId": "1701d744-2a75-4037-e4eb-e6174b5f7df0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4175183ea9a41c996a02690c598e670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avo9\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\avo9\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba67a77f5ab41cbb7e2f42389e552e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d75d71b7e24be7997803f86963a1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281bec2a9ee544fd959d674f8dbecbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad04306d03414954b30321d132ce3b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Dr', '.', 'ĠSmith', 'âĢ', 'Ļ', 's', 'ĠAI', '-', 'powered', 'Ġmodel', 'Ġ(', 'trained', 'Ġon', 'ĠT', 'Ã¼r', 'k', 'Ã§', 'e', ',', 'Ġ10', '+', 'Ġlanguages', ')', 'Ġoutper', 'formed', 'Ġothers', 'Ġin', 'Ġthe', \"Ġ'\", '20', '23', '_', 'Chall', 'enge', \"'\", '!', 'Ġ#', 'N', 'LP', '.']\n",
      "Token IDs: [6187, 13, 4176, 447, 247, 82, 9552, 12, 12293, 2746, 357, 35311, 319, 309, 25151, 74, 16175, 68, 11, 838, 10, 8950, 8, 33597, 12214, 1854, 287, 262, 705, 1238, 1954, 62, 41812, 3540, 6, 0, 1303, 45, 19930, 13]\n"
     ]
    }
   ],
   "source": [
    "tokenizer3 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer3.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "# Tokenize the text\n",
    "tokens = tokenizer3.tokenize(text)\n",
    "token_ids = tokenizer3.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print tokens and IDs\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "37e1b46537c94ab5a4837532e3850f3b",
      "33c35c376b5244d7a2debf6e24a22696",
      "e0185eacbaaa4a629551c7251a2a0f4f",
      "2d1c6274f8694e6e95edb8dc61aaccf1",
      "e2b25e9f6c8f4de6a30cb783021aab0c",
      "ded7eace28f547fb858dcf8686c3f27a",
      "62a8a2c15aa84eceb6a79c33a69db295",
      "db89592028794381ad58de5db2bd18e8",
      "35a43dc63073425ca01e0df24d807257",
      "8d951407815b42e482b6220061519f2c",
      "6e62c43e2ee443e5a6f4d6e7fc50dfff",
      "8044d7f5be854e9e98a68f3d03f17f38",
      "10b8454f62e6477a8dec59ec7a7d3d6c",
      "a7e0d95957bb42599aaaf89f58022363",
      "2f630e38c63a4040b6d3379c02c90fa7",
      "e00136f7c3e144c3bf4698afda07dfe9",
      "811cbaefeefc4f2a935d424e06180621",
      "aafb6cbc0f5848d1b67930d93f752f02",
      "53a62818dfe542828929bf510244021a",
      "25a4144ac6bf4a1eb33f0004250b67ff",
      "bf85da8f1e9d4617802ad2c4a42f2942",
      "cdde64d061ac4235a2c2fea679f1902b",
      "872e2c7e142a41a7ab0b74eab9e76d26",
      "6904d60ee9734660b706c3f8fac184b9",
      "f0095527240043d5b55a4964749534cc",
      "a4ccdb4266d04c7db1799a652b4b83a0",
      "961aca1b53e74f3785f4dfa569244bcc",
      "7ea0d564fdc24843bcfa1e681d401ffc",
      "0b356895045547b69b1d2d65dd1fe2ac",
      "c00ee18859074fbcb3649cd00ed59c68",
      "59df84cfe05645ec93be007919ce3be8",
      "7f814395e1274d1981b270db23189bec",
      "10a73427de81425b8c56afacd8b3e9f4"
     ]
    },
    "id": "LJY8JIV-uXLM",
    "outputId": "341b638e-01f8-44df-dc6d-e31f288dbfb5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f58aaec2c64dcf94415e2be272a648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avo9\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\avo9\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d01662b018c48f48f6fc4cb8c331f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c401a47b96f45e48bcf8fc9b74dd1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁Dr', '.', '▁Smith', '’', 's', '▁AI', '-', 'powered', '▁model', '▁(', 't', 'rained', '▁on', '▁Tür', 'k', 'ç', 'e', ',', '▁10', '+', '▁languages', ')', '▁out', 'per', 'formed', '▁others', '▁in', '▁the', '▁', \"'\", '20', '23', '_', 'C', 'hall', 'en', 'ge', \"'\", '!', '▁#', 'N', 'LP', '.']\n",
      "Token IDs: [707, 5, 3931, 22, 7, 7833, 18, 17124, 825, 41, 17, 10761, 30, 12087, 157, 8970, 15, 6, 335, 1220, 8024, 61, 91, 883, 10816, 717, 16, 8, 3, 31, 1755, 2773, 834, 254, 11516, 35, 397, 31, 55, 1713, 567, 6892, 5]\n"
     ]
    }
   ],
   "source": [
    "tokenizer4 = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer4.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "# Tokenize the text\n",
    "tokens = tokenizer4.tokenize(text)\n",
    "token_ids = tokenizer4.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print tokens and IDs\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "0fffc30b2c2e4ea9a267a1b3b5589b33",
      "2040caf96d1f4ae4ab039d29513d6c91",
      "de0e654893b44663947da092f1676e0e",
      "fd9a074045e045ffa43cb9a6d47060e8",
      "74be30a58fbf4ac9bc4984bdca3b5e08",
      "c2f5f9ba08004aa19ce6120bbeeff3e1",
      "d396f3e93c92432fbf7bb9c0d8a8dede",
      "2d3d23981191493cb20c621fcfc6b592",
      "a64319bfb0ab4765a583b4f78b68b91d",
      "7872239230ae45e79d45fbaaa90d4d84",
      "c6d5f36a8c0643b08ba487cc1b126831",
      "4e6e82515dfa48eda232550a4f0a994e",
      "073e103250784e198428f1e0360509b7",
      "c7dc2958fea84781ac43ede1816e5f0e",
      "03c4343d4bdf44379488d78df6897cc8",
      "d460ccb476614587a83ba4273abbc1f9",
      "a159a5829a4d4450a21767f1dd270d20",
      "ecf3733b5b5e4135b780ce71c613c6b0",
      "c0ff8a9d0ddc4506bd1bc38d29c252d9",
      "97f783472b2d4991aa1feb3fb3174c5d",
      "050605db4ff14a6aa4e4a630b5b23ddf",
      "b9891d43c9a944a1b97a42dbb9831b27"
     ]
    },
    "id": "ItWeYNgbuhcL",
    "outputId": "9176215b-dc5a-463c-f2ee-ee4ebb7d2b14"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf4253c4a3744a6a2f58e946aebc871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avo9\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\avo9\\.cache\\huggingface\\hub\\models--deepseek-ai--deepseek-r1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1293c9d4a4e44588ea24bb56ecc4fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Dr', '.', 'ĠSmith', 'âĢĻ', 's', 'ĠAI', '-powered', 'Ġmodel', 'Ġ(', 't', 'rained', 'Ġon', 'ĠTÃ¼r', 'k', 'Ã§', 'e', ',', 'Ġ', '10', '+', 'Ġlanguages', ')', 'Ġoutper', 'formed', 'Ġothers', 'Ġin', 'Ġthe', \"Ġ'\", '202', '3', '_', 'Challenge', \"'\", '!', 'Ġ#', 'N', 'LP', '.']\n",
      "Token IDs: [12528, 16, 10201, 442, 85, 7703, 42793, 2645, 343, 86, 17021, 377, 82402, 77, 2341, 71, 14, 223, 553, 13, 10555, 11, 55006, 19886, 3628, 295, 270, 905, 939, 21, 65, 99042, 9, 3, 1823, 48, 23925, 16]\n"
     ]
    }
   ],
   "source": [
    "#how about deepseek?\n",
    "\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-r1\")\n",
    "tokenizer4.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "# Tokenize the text\n",
    "tokens = tokenizer4.tokenize(text)\n",
    "token_ids = tokenizer4.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print tokens and IDs\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
