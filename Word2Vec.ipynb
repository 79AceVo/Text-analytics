{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br5pHUoZtqny"
      },
      "source": [
        "# Word Embeddings using Word2Vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "siTp_wObnj3P"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as gen_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G5_qC_A2nshz"
      },
      "outputs": [],
      "source": [
        "gen_info = gen_api.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KummJYlmn5Pz",
        "outputId": "f9e62a3f-d54b-4c28-fae5-377109583804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fasttext-wiki-news-subwords-300\n",
            "1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
            "\n",
            "conceptnet-numberbatch-17-06-300\n",
            "ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.\n",
            "\n",
            "word2vec-ruscorpora-300\n",
            "Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.\n",
            "\n",
            "word2vec-google-news-300\n",
            "Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\n",
            "\n",
            "glove-wiki-gigaword-50\n",
            "Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "\n",
            "glove-wiki-gigaword-100\n",
            "Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "\n",
            "glove-wiki-gigaword-200\n",
            "Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "\n",
            "glove-wiki-gigaword-300\n",
            "Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "\n",
            "glove-twitter-25\n",
            "Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "\n",
            "glove-twitter-50\n",
            "Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
            "\n",
            "glove-twitter-100\n",
            "Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
            "\n",
            "glove-twitter-200\n",
            "Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
            "\n",
            "__testing_word2vec-matrix-synopsis\n",
            "[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for model_name, model_data in gen_info[\"models\"].items():\n",
        "  print(model_name)\n",
        "  print(model_data[\"description\"])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyId8CKNon0_"
      },
      "source": [
        "In Gensim, there are 4 big type: Fasttext, ConceptNet, Word2Vec, and GloVe. You can see that the algorithm is one thing, but the corpus training is another. Each corpus training will give you a slightly different bend, depending on the context.\n",
        "\n",
        "FYI uncased means everything has been lower case. (uncased == no case)\n",
        "\n",
        "First we need to load the algorithm for the word embedding. This is also called pre-trained model.\n",
        "\n",
        "You can download the pre-trained Word2Vec model from Google here https://code.google.com/p/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhjQH0PEn7jD",
        "outputId": "1649e8d9-c4b4-4885-977c-45a41dfccc60"
      },
      "outputs": [],
      "source": [
        "#let's load word2vec Google News 300. Takes a while. 1.6GB\n",
        "w2v = gen_api.load ('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr31Msi6r3dS"
      },
      "source": [
        "After loading, let's see how they represent texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNI72Ys7prc7",
        "outputId": "c7108be0-bcf1-4b56-a047-81f2f521cb6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('coffees', 0.721267819404602),\n",
              " ('gourmet_coffee', 0.7057086825370789),\n",
              " ('Coffee', 0.6900454759597778),\n",
              " ('o_joe', 0.6891065835952759),\n",
              " ('Starbucks_coffee', 0.6874972581863403),\n",
              " ('coffee_beans', 0.6749704480171204),\n",
              " ('lattÃ©', 0.664122462272644),\n",
              " ('cappuccino', 0.662549614906311),\n",
              " ('brewed_coffee', 0.6621608138084412),\n",
              " ('espresso', 0.6616826057434082)]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.most_similar(\"coffee\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0SPRkvaBn-5",
        "outputId": "55d9e3d1-719f-4c51-dd79-cace986e875f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.61132812e-01, -1.36718750e-01, -3.73046875e-01,  6.17187500e-01,\n",
              "        1.08398438e-01,  2.72216797e-02,  1.00097656e-01, -1.51367188e-01,\n",
              "       -1.66015625e-02,  3.80859375e-01,  6.54296875e-02, -1.31835938e-01,\n",
              "        2.53906250e-01,  9.08203125e-02,  2.86865234e-02,  2.53906250e-01,\n",
              "       -2.05078125e-01,  1.64062500e-01,  2.20703125e-01, -1.74804688e-01,\n",
              "       -2.01171875e-01,  1.30859375e-01, -3.22265625e-02, -2.41210938e-01,\n",
              "       -3.19824219e-02,  2.48046875e-01, -2.37304688e-01,  2.89062500e-01,\n",
              "        1.64794922e-02,  1.29394531e-02,  1.72119141e-02, -3.53515625e-01,\n",
              "       -1.66992188e-01, -5.90820312e-02, -2.81250000e-01,  9.94873047e-03,\n",
              "       -1.94091797e-02, -3.22265625e-01,  1.73339844e-02, -5.83496094e-02,\n",
              "       -2.59765625e-01,  1.42669678e-03,  5.81054688e-02,  1.13769531e-01,\n",
              "       -8.64257812e-02,  3.54003906e-02, -4.29687500e-01,  2.86865234e-03,\n",
              "        6.98852539e-03,  1.80664062e-01, -1.79687500e-01,  2.95410156e-02,\n",
              "       -1.56250000e-01, -2.08007812e-01, -9.08203125e-02,  4.15039062e-03,\n",
              "        1.07421875e-01,  3.12500000e-01, -1.04980469e-01, -3.24218750e-01,\n",
              "       -1.24023438e-01, -7.05718994e-04, -1.05957031e-01,  2.12890625e-01,\n",
              "        1.12304688e-01, -1.58203125e-01, -1.67968750e-01, -9.71679688e-02,\n",
              "        1.53320312e-01, -1.11328125e-01,  3.22265625e-01,  2.28515625e-01,\n",
              "        3.20312500e-01, -1.72119141e-02, -4.57031250e-01,  3.23486328e-03,\n",
              "       -1.76757812e-01, -5.00488281e-02,  3.05175781e-02, -2.75390625e-01,\n",
              "       -1.65039062e-01, -3.56445312e-02,  7.95898438e-02,  1.35742188e-01,\n",
              "       -8.64257812e-02, -7.32421875e-02,  1.36718750e-01,  2.33398438e-01,\n",
              "        7.95898438e-02,  1.32446289e-02, -4.71191406e-02,  1.01074219e-01,\n",
              "        2.37304688e-01, -1.81640625e-01, -2.14843750e-01, -1.65039062e-01,\n",
              "       -1.66015625e-02, -1.51367188e-01,  3.06640625e-01, -2.40234375e-01,\n",
              "       -2.29492188e-01, -1.29882812e-01,  8.97216797e-03,  1.97265625e-01,\n",
              "        7.47070312e-02, -1.64031982e-03,  1.54296875e-01, -6.80541992e-03,\n",
              "       -1.12304688e-01, -7.61718750e-02, -8.74023438e-02, -1.31835938e-01,\n",
              "       -2.94921875e-01, -2.46093750e-01,  6.15234375e-02, -1.23046875e-01,\n",
              "       -8.34960938e-02, -8.39843750e-02, -1.61132812e-02, -4.30297852e-03,\n",
              "       -4.05273438e-02, -2.84423828e-02,  1.36718750e-01,  2.13623047e-02,\n",
              "       -2.81250000e-01,  2.40234375e-01, -3.75976562e-02, -9.66796875e-02,\n",
              "        1.28906250e-01,  1.43554688e-01, -1.37695312e-01, -1.38549805e-02,\n",
              "       -4.12597656e-02, -4.51660156e-02, -3.75976562e-02,  1.89453125e-01,\n",
              "        5.32226562e-02,  1.17675781e-01, -8.25195312e-02, -1.56250000e-01,\n",
              "        1.47460938e-01, -2.63671875e-01, -2.79296875e-01, -4.31640625e-01,\n",
              "       -5.90820312e-02,  2.74658203e-03,  2.87109375e-01, -2.71606445e-03,\n",
              "       -2.46093750e-01,  2.74658203e-02, -9.08203125e-02,  6.54296875e-02,\n",
              "       -1.94335938e-01, -2.16064453e-02,  2.77343750e-01,  5.98144531e-02,\n",
              "        2.33154297e-02, -1.37695312e-01, -5.39062500e-01, -1.64794922e-02,\n",
              "       -1.25976562e-01, -1.36718750e-01,  3.02734375e-02,  2.50000000e-01,\n",
              "        5.53131104e-04,  1.36718750e-01,  2.96875000e-01, -5.10253906e-02,\n",
              "        9.08203125e-02, -2.39257812e-01,  1.35742188e-01,  1.11328125e-01,\n",
              "        1.96289062e-01, -1.54296875e-01, -3.37890625e-01, -3.36914062e-02,\n",
              "       -9.47265625e-02, -1.69921875e-01, -1.04003906e-01,  1.46484375e-01,\n",
              "        4.54101562e-02, -4.12109375e-01, -2.47070312e-01, -6.10351562e-03,\n",
              "        4.55078125e-01, -2.35595703e-02,  4.93164062e-02,  1.42578125e-01,\n",
              "        2.66113281e-02,  4.11987305e-03, -7.27539062e-02,  2.53906250e-02,\n",
              "       -3.39355469e-02,  7.91015625e-02,  2.87109375e-01,  3.88671875e-01,\n",
              "       -1.58691406e-02, -8.44726562e-02, -1.15722656e-01, -1.22558594e-01,\n",
              "       -1.02050781e-01,  1.32812500e-01,  2.21679688e-01, -2.03125000e-01,\n",
              "        7.91015625e-02,  1.69677734e-02,  2.16796875e-01,  2.33398438e-01,\n",
              "       -2.08984375e-01, -1.36718750e-01, -2.45117188e-01,  3.93066406e-02,\n",
              "       -1.80664062e-01,  1.37695312e-01,  1.50390625e-01, -3.90625000e-02,\n",
              "       -1.32812500e-01,  2.75878906e-02, -1.78710938e-01,  1.55273438e-01,\n",
              "        1.36718750e-01, -1.14257812e-01, -2.79296875e-01, -7.86132812e-02,\n",
              "        3.08593750e-01, -5.32226562e-02, -1.65039062e-01,  5.83496094e-02,\n",
              "        2.19726562e-01, -1.25000000e-01,  6.10351562e-02, -3.39355469e-02,\n",
              "       -3.16406250e-01,  2.14843750e-01, -4.12597656e-02, -1.94335938e-01,\n",
              "        7.76367188e-02, -5.21850586e-03,  6.93359375e-02,  2.18750000e-01,\n",
              "        1.71875000e-01, -1.97265625e-01,  1.07910156e-01,  8.25195312e-02,\n",
              "        3.39355469e-02, -1.15722656e-01, -2.02941895e-03,  4.83398438e-02,\n",
              "        1.50390625e-01, -2.73437500e-01, -9.61914062e-02,  3.39843750e-01,\n",
              "        2.98828125e-01,  1.32812500e-01, -3.68652344e-02, -3.08593750e-01,\n",
              "        2.94189453e-02, -1.31835938e-01, -7.12890625e-02, -2.57873535e-03,\n",
              "       -1.17187500e-01,  6.34765625e-03, -1.66992188e-01,  2.01171875e-01,\n",
              "       -1.33789062e-01, -1.77734375e-01, -1.09863281e-01,  5.06591797e-03,\n",
              "       -1.07910156e-01, -1.30859375e-01, -5.17578125e-02,  2.57812500e-01,\n",
              "        5.41992188e-02, -6.34765625e-03,  3.00598145e-03,  7.95898438e-02,\n",
              "       -2.37304688e-01, -8.05664062e-02,  6.07910156e-02,  9.27734375e-02,\n",
              "        1.65039062e-01, -1.22558594e-01,  1.88476562e-01,  2.50000000e-01,\n",
              "       -1.42578125e-01, -7.91015625e-02, -1.78710938e-01,  1.52343750e-01,\n",
              "       -7.76367188e-02,  2.42187500e-01,  2.56347656e-02, -1.26953125e-01,\n",
              "       -1.25000000e-01, -3.19824219e-02, -1.27929688e-01,  1.49414062e-01,\n",
              "       -1.34277344e-02,  6.59179688e-02,  2.17773438e-01,  2.02148438e-01],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v['coffee'] #What's the vectorization of the word?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgDzNV_pBzQc",
        "outputId": "573ce9cd-5b92-410c-e024-f4704e33291f"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"Key 'brexit' not present\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w2v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrexit\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\avo9\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key_or_keys)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
            "File \u001b[1;32mc:\\Users\\avo9\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
            "File \u001b[1;32mc:\\Users\\avo9\\AppData\\Local\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"Key 'brexit' not present\""
          ]
        }
      ],
      "source": [
        "w2v['brexit'] #What's the vectorization of the word that outside of the library?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using Cosine similarity to see how similar\n",
        "\n",
        "Higher = more similar with cosine_similarity.\n",
        "\n",
        "1.0 = identical\n",
        "0.0 = completely unrelated\n",
        "-1.0 = opposite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tzUAnbWjrwpy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7535746246576309"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.distance(\"coffee\",\"cream\") #similar things should be closer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.24642539]], dtype=float32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1 = w2v[\"coffee\"]\n",
        "text2 = w2v[\"cream\"]\n",
        "text3 = w2v[\"basketball\"]\n",
        "cosine_similarity([text1], [text2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ip-fBcLMr90R"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9099433422088623"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.distance(\"coffee\",\"basketball\") #diferrent things should be different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.09005667]], dtype=float32)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity([text1], [text3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ei2O4NiasID1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('queen', 0.9834331274032593),\n",
              " ('monarch', 0.954605221748352),\n",
              " ('princess', 0.9464353322982788),\n",
              " ('ruler', 0.9094336628913879),\n",
              " ('prince', 0.8853203654289246),\n",
              " ('crown_prince', 0.8740110993385315),\n",
              " ('maharaja', 0.8640182614326477),\n",
              " ('sultan', 0.8614116311073303),\n",
              " ('King_Ahasuerus', 0.8606166839599609),\n",
              " ('Queen_Consort', 0.845000147819519)]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#wanna try king - man + woman?\n",
        "w2v.most_similar_cosmul(positive=['king','woman'], negative=['men']) #function on the most word that has similar cosine similarity, a similarity metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rnW5rWPIsxLe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('eatery', 0.8693193793296814),\n",
              " ('bartender', 0.8536876440048218),\n",
              " ('bartenders', 0.8526809811592102),\n",
              " ('nightspot', 0.8493297100067139),\n",
              " ('Buddha_Bar', 0.8486438393592834),\n",
              " ('Pegu_Club', 0.8456864953041077),\n",
              " ('brewpub', 0.8379691243171692),\n",
              " ('La_Floridita', 0.836773157119751),\n",
              " ('cafe', 0.8341168165206909),\n",
              " ('Tres_Agaves', 0.830361545085907)]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#guess this one first before you run\n",
        "w2v.most_similar_cosmul(positive=['restaurant','cocktail'], negative=['dinner']) #function on the most word that has similar cosine similarity, a similarity metric"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
