{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Step-by-Step: Processing \"I love NLP\"\n",
    "\n",
    "## Setup\n",
    "- Sentence: **\"I love NLP\"**\n",
    "- Hidden units: **4** (per direction)\n",
    "- Embedding dimension: **3**\n",
    "- **Same as RNN and LSTM examples for comparison!**\n",
    "\n",
    "## Bidirectional LSTM (BiLSTM)\n",
    "- Processes sequence **FORWARD** (left-to-right): I → love → NLP\n",
    "- Processes sequence **BACKWARD** (right-to-left): NLP → love → I\n",
    "- **Concatenates** both directions at each time step\n",
    "- Output dimension: **8** (4 forward + 4 backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration (SAME as RNN and LSTM)\n",
    "vocab = {\"I\": 0, \"love\": 1, \"NLP\": 2}\n",
    "embedding_dim = 3\n",
    "hidden_units = 4  # Per direction\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Hidden units per direction: {hidden_units}\")\n",
    "print(f\"Total output dimension: {hidden_units * 2} (forward + backward)\")\n",
    "print()\n",
    "print(\"BiLSTM has TWO separate LSTMs:\")\n",
    "print(\"  1. Forward LSTM:  processes I → love → NLP\")\n",
    "print(\"  2. Backward LSTM: processes NLP → love → I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Embedding Matrix (SAME as RNN/LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding matrix (SAME as RNN and LSTM examples)\n",
    "embedding_matrix = np.array([\n",
    "    [0.5, 0.2, 0.1],   # \"I\"\n",
    "    [0.8, 0.6, 0.3],   # \"love\"\n",
    "    [0.1, 0.9, 0.7]    # \"NLP\"\n",
    "])\n",
    "\n",
    "print(\"Embedding Matrix:\")\n",
    "print(embedding_matrix)\n",
    "\n",
    "# Get embeddings for our sentence\n",
    "sentence = [\"I\", \"love\", \"NLP\"]\n",
    "sentence_ids = [vocab[word] for word in sentence]\n",
    "embeddings = embedding_matrix[sentence_ids]\n",
    "\n",
    "print(\"\\nWord Embeddings:\")\n",
    "for i, word in enumerate(sentence):\n",
    "    print(f\"{word:6s}: {embeddings[i]}\")\n",
    "\n",
    "print(\"\\nForward sequence:  I (idx=0) → love (idx=1) → NLP (idx=2)\")\n",
    "print(\"Backward sequence: NLP (idx=2) → love (idx=1) → I (idx=0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Weights for BOTH Directions\n",
    "\n",
    "BiLSTM has **TWO sets of weights** (one for forward, one for backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "# Simplified LSTM weights (using same structure as LSTM example)\n",
    "# FORWARD LSTM weights\n",
    "np.random.seed(42)\n",
    "W_f_fwd_x = np.random.randn(hidden_units, embedding_dim) * 0.1\n",
    "W_f_fwd_h = np.random.randn(hidden_units, hidden_units) * 0.1\n",
    "b_f_fwd = np.ones(hidden_units) * 0.5\n",
    "\n",
    "W_i_fwd_x = np.random.randn(hidden_units, embedding_dim) * 0.1\n",
    "W_i_fwd_h = np.random.randn(hidden_units, hidden_units) * 0.1\n",
    "b_i_fwd = np.ones(hidden_units) * 0.1\n",
    "\n",
    "W_C_fwd_x = np.random.randn(hidden_units, embedding_dim) * 0.1\n",
    "W_C_fwd_h = np.random.randn(hidden_units, hidden_units) * 0.1\n",
    "b_C_fwd = np.ones(hidden_units) * 0.1\n",
    "\n",
    "W_o_fwd_x = np.random.randn(hidden_units, embedding_dim) * 0.1\n",
    "W_o_fwd_h = np.random.randn(hidden_units, hidden_units) * 0.1\n",
    "b_o_fwd = np.ones(hidden_units) * 0.1\n",
    "\n",
    "# BACKWARD LSTM weights (different from forward)\n",
    "np.random.seed(123)\n",
    "W_f_bwd_x = np.random.randn(hidden_units, embedding_dim) * 0.1\n",
    "W_f_bwd_h = np.random.randn(hidden_units, hidden_units) * 0.1\n",
    "b_f_bwd = np.ones(hidden_units) * 0.5\n",
    "\n",
    "W_i_bwd_x = np.random.randn(hidden_units, embedding_dim) * 0.1\n",
    "W_i_bwd_h = np.random.randn(hidden_units, hidden_units) * 0.1\n",
    "b_i_bwd = np.ones(hidden_units) * 0.1\n",
    "\n",
    "W_C_bwd_x = np.random.randn(hidden_units, embedding_dim) * 0.1\n",
    "W_C_bwd_h = np.random.randn(hidden_units, hidden_units) * 0.1\n",
    "b_C_bwd = np.ones(hidden_units) * 0.1\n",
    "\n",
    "W_o_bwd_x = np.random.randn(hidden_units, embedding_dim) * 0.1\n",
    "W_o_bwd_h = np.random.randn(hidden_units, hidden_units) * 0.1\n",
    "b_o_bwd = np.ones(hidden_units) * 0.1\n",
    "\n",
    "print(\"BiLSTM Weight Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Forward LSTM:  4 gates × weights = separate parameters\")\n",
    "print(\"Backward LSTM: 4 gates × weights = separate parameters\")\n",
    "print()\n",
    "print(\"Total parameters ≈ 2× regular LSTM\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: LSTM Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_step(x_t, h_prev, C_prev, W_f_x, W_f_h, b_f, W_i_x, W_i_h, b_i,\n",
    "              W_C_x, W_C_h, b_C, W_o_x, W_o_h, b_o):\n",
    "    \"\"\"Single LSTM forward step\"\"\"\n",
    "    \n",
    "    # Forget gate\n",
    "    f_t = sigmoid(W_f_x.T @ x_t + W_f_h.T @ h_prev + b_f)\n",
    "    \n",
    "    # Input gate\n",
    "    i_t = sigmoid(W_i_x.T @ x_t + W_i_h.T @ h_prev + b_i)\n",
    "    \n",
    "    # Candidate cell state\n",
    "    C_tilde = np.tanh(W_C_x.T @ x_t + W_C_h.T @ h_prev + b_C)\n",
    "    \n",
    "    # New cell state\n",
    "    C_t = f_t * C_prev + i_t * C_tilde\n",
    "    \n",
    "    # Output gate\n",
    "    o_t = sigmoid(W_o_x.T @ x_t + W_o_h.T @ h_prev + b_o)\n",
    "    \n",
    "    # New hidden state\n",
    "    h_t = o_t * np.tanh(C_t)\n",
    "    \n",
    "    return h_t, C_t\n",
    "\n",
    "print(\"LSTM step function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: FORWARD LSTM Pass (Left-to-Right)\n",
    "\n",
    "Process: **I → love → NLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FORWARD LSTM PASS (I → love → NLP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize forward states\n",
    "h_fwd = [np.zeros(hidden_units)]  # h_0\n",
    "C_fwd = [np.zeros(hidden_units)]  # C_0\n",
    "\n",
    "print(\"Initial state:\")\n",
    "print(f\"h_fwd_0: {h_fwd[0]}\")\n",
    "print(f\"C_fwd_0: {C_fwd[0]}\")\n",
    "print()\n",
    "\n",
    "# Process each word in FORWARD direction\n",
    "for i, word in enumerate(sentence):\n",
    "    x_t = embeddings[i]\n",
    "    h_prev = h_fwd[-1]\n",
    "    C_prev = C_fwd[-1]\n",
    "    \n",
    "    print(f\"Time step {i+1}: Processing '{word}'\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Input x_{i+1}: {x_t}\")\n",
    "    \n",
    "    h_t, C_t = lstm_step(x_t, h_prev, C_prev,\n",
    "                        W_f_fwd_x, W_f_fwd_h, b_f_fwd,\n",
    "                        W_i_fwd_x, W_i_fwd_h, b_i_fwd,\n",
    "                        W_C_fwd_x, W_C_fwd_h, b_C_fwd,\n",
    "                        W_o_fwd_x, W_o_fwd_h, b_o_fwd)\n",
    "    \n",
    "    h_fwd.append(h_t)\n",
    "    C_fwd.append(C_t)\n",
    "    \n",
    "    print(f\"h_fwd_{i+1}: {h_t}\")\n",
    "    print(f\"C_fwd_{i+1}: {C_t}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Forward LSTM complete!\")\n",
    "print(f\"Final forward hidden state h_fwd_3: {h_fwd[3]}\")\n",
    "print(f\"  → Encodes information from 'I love NLP' (left-to-right)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: BACKWARD LSTM Pass (Right-to-Left)\n",
    "\n",
    "Process: **NLP → love → I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBACKWARD LSTM PASS (NLP → love → I)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize backward states\n",
    "h_bwd = [np.zeros(hidden_units)]  # h_0\n",
    "C_bwd = [np.zeros(hidden_units)]  # C_0\n",
    "\n",
    "print(\"Initial state:\")\n",
    "print(f\"h_bwd_0: {h_bwd[0]}\")\n",
    "print(f\"C_bwd_0: {C_bwd[0]}\")\n",
    "print()\n",
    "\n",
    "# Process each word in BACKWARD direction (reverse order)\n",
    "for i, word in enumerate(reversed(sentence)):\n",
    "    idx = len(sentence) - 1 - i  # Actual index in original sequence\n",
    "    x_t = embeddings[idx]\n",
    "    h_prev = h_bwd[-1]\n",
    "    C_prev = C_bwd[-1]\n",
    "    \n",
    "    print(f\"Time step {i+1}: Processing '{word}' (from position {idx})\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Input x_{idx}: {x_t}\")\n",
    "    \n",
    "    h_t, C_t = lstm_step(x_t, h_prev, C_prev,\n",
    "                        W_f_bwd_x, W_f_bwd_h, b_f_bwd,\n",
    "                        W_i_bwd_x, W_i_bwd_h, b_i_bwd,\n",
    "                        W_C_bwd_x, W_C_bwd_h, b_C_bwd,\n",
    "                        W_o_bwd_x, W_o_bwd_h, b_o_bwd)\n",
    "    \n",
    "    h_bwd.append(h_t)\n",
    "    C_bwd.append(C_t)\n",
    "    \n",
    "    print(f\"h_bwd_{i+1}: {h_t}\")\n",
    "    print(f\"C_bwd_{i+1}: {C_t}\")\n",
    "    print()\n",
    "\n",
    "# Reverse the backward states to align with forward\n",
    "h_bwd_aligned = [h_bwd[0]] + list(reversed(h_bwd[1:]))\n",
    "C_bwd_aligned = [C_bwd[0]] + list(reversed(C_bwd[1:]))\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Backward LSTM complete!\")\n",
    "print(f\"Final backward hidden state h_bwd_3: {h_bwd[3]}\")\n",
    "print(f\"  → Encodes information from 'I love NLP' (right-to-left)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Concatenate Forward and Backward States\n",
    "\n",
    "At each time step, combine both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCONCATENATING FORWARD AND BACKWARD STATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Concatenate hidden states at each time step\n",
    "h_bilstm = []\n",
    "for i in range(len(sentence) + 1):\n",
    "    h_combined = np.concatenate([h_fwd[i], h_bwd_aligned[i]])\n",
    "    h_bilstm.append(h_combined)\n",
    "\n",
    "print(\"BiLSTM outputs at each time step:\")\n",
    "print()\n",
    "for i, word in enumerate(['(init)', 'I', 'love', 'NLP']):\n",
    "    print(f\"t={i} ({word}):\")\n",
    "    print(f\"  Forward:  {h_fwd[i]}\")\n",
    "    print(f\"  Backward: {h_bwd_aligned[i]}\")\n",
    "    print(f\"  Combined: {h_bilstm[i]}\")\n",
    "    print(f\"  Shape: {h_bilstm[i].shape} (4 forward + 4 backward = 8 total)\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Key Insight:\")\n",
    "print(\"At each position, BiLSTM has context from BOTH directions:\")\n",
    "print(\"  - Forward: what came BEFORE\")\n",
    "print(\"  - Backward: what comes AFTER\")\n",
    "print()\n",
    "print(\"Example at 'love' (t=2):\")\n",
    "print(\"  Forward state knows: 'I love'\")\n",
    "print(\"  Backward state knows: 'love NLP'\")\n",
    "print(\"  → BiLSTM knows FULL sentence context!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Directional Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(8, 7.5, 'BiLSTM: Bidirectional Processing of \"I love NLP\"', \n",
    "        ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Time step positions\n",
    "x_positions = [3, 6, 9, 12]\n",
    "words = ['(init)', 'I', 'love', 'NLP']\n",
    "\n",
    "# Draw forward path (top)\n",
    "y_fwd = 5.5\n",
    "for i, (x, word) in enumerate(zip(x_positions, words)):\n",
    "    # Forward neuron cluster\n",
    "    if i > 0:\n",
    "        for j in range(4):\n",
    "            y_offset = y_fwd + (j - 1.5) * 0.2\n",
    "            circle = plt.Circle((x, y_offset), 0.12, color='#5B9BD5', \n",
    "                              ec='black', linewidth=1.5, zorder=3)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        ax.text(x, y_fwd - 0.6, word, ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Forward arrows\n",
    "    if i < len(x_positions) - 1:\n",
    "        ax.arrow(x + 0.4, y_fwd, x_positions[i+1] - x - 0.9, 0,\n",
    "                head_width=0.2, head_length=0.2, fc='#5B9BD5', ec='#5B9BD5', \n",
    "                linewidth=3, zorder=2)\n",
    "\n",
    "ax.text(1.5, y_fwd, 'FORWARD →', ha='center', fontsize=12, \n",
    "        fontweight='bold', color='#5B9BD5')\n",
    "\n",
    "# Draw backward path (bottom)\n",
    "y_bwd = 2.5\n",
    "for i, (x, word) in enumerate(zip(x_positions, words)):\n",
    "    # Backward neuron cluster\n",
    "    if i > 0:\n",
    "        for j in range(4):\n",
    "            y_offset = y_bwd + (j - 1.5) * 0.2\n",
    "            circle = plt.Circle((x, y_offset), 0.12, color='#E67E22',\n",
    "                              ec='black', linewidth=1.5, zorder=3)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        ax.text(x, y_bwd + 0.6, word, ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Backward arrows (right to left)\n",
    "    if i > 0:\n",
    "        ax.arrow(x - 0.4, y_bwd, -(x - x_positions[i-1] - 0.9), 0,\n",
    "                head_width=0.2, head_length=0.2, fc='#E67E22', ec='#E67E22',\n",
    "                linewidth=3, zorder=2)\n",
    "\n",
    "ax.text(14.5, y_bwd, '← BACKWARD', ha='center', fontsize=12,\n",
    "        fontweight='bold', color='#E67E22')\n",
    "\n",
    "# Draw concatenation\n",
    "for i, x in enumerate(x_positions[1:], 1):\n",
    "    # Vertical connector\n",
    "    ax.plot([x, x], [y_fwd - 0.5, y_bwd + 0.5], 'k--', linewidth=2, alpha=0.5)\n",
    "    \n",
    "    # Combined output\n",
    "    rect = Rectangle((x - 0.4, 4 - 0.3), 0.8, 0.6, \n",
    "                     facecolor='#2ECC71', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, 4, f'BiLSTM\\n({hidden_units*2})', ha='center', va='center',\n",
    "           fontsize=9, fontweight='bold', color='white')\n",
    "\n",
    "# Legend\n",
    "ax.text(1, 1, 'Forward (4 units): Past context', fontsize=10, color='#5B9BD5')\n",
    "ax.text(1, 0.6, 'Backward (4 units): Future context', fontsize=10, color='#E67E22')\n",
    "ax.text(1, 0.2, 'Combined (8 units): Full context', fontsize=10, color='#2ECC71')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bilstm_directional_flow.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'bilstm_directional_flow.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: State Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmaps\n",
    "h_fwd_array = np.array(h_fwd[1:])  # Skip initial zero state\n",
    "h_bwd_array = np.array([h_bwd_aligned[1], h_bwd_aligned[2], h_bwd_aligned[3]])\n",
    "h_bilstm_array = np.array(h_bilstm[1:])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Forward hidden states\n",
    "sns.heatmap(h_fwd_array, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=[f'Unit {i+1}' for i in range(4)],\n",
    "            yticklabels=['I', 'love', 'NLP'],\n",
    "            ax=axes[0], cbar_kws={'label': 'Activation'})\n",
    "axes[0].set_title('Forward LSTM\\n(I → love → NLP)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Time Step', fontsize=11)\n",
    "\n",
    "# Backward hidden states\n",
    "sns.heatmap(h_bwd_array, annot=True, fmt='.2f', cmap='Oranges',\n",
    "            xticklabels=[f'Unit {i+1}' for i in range(4)],\n",
    "            yticklabels=['I', 'love', 'NLP'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Activation'})\n",
    "axes[1].set_title('Backward LSTM\\n(NLP → love → I)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Time Step', fontsize=11)\n",
    "\n",
    "# BiLSTM combined\n",
    "sns.heatmap(h_bilstm_array, annot=True, fmt='.2f', cmap='Greens',\n",
    "            xticklabels=[f'Fwd{i+1}' for i in range(4)] + [f'Bwd{i+1}' for i in range(4)],\n",
    "            yticklabels=['I', 'love', 'NLP'],\n",
    "            ax=axes[2], cbar_kws={'label': 'Activation'})\n",
    "axes[2].set_title('BiLSTM Combined\\n(Forward + Backward)', fontsize=13, fontweight='bold')\n",
    "axes[2].set_ylabel('Time Step', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bilstm_state_heatmaps.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'bilstm_state_heatmaps.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Context at Each Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(7, 7.5, 'BiLSTM: Context Available at Each Position', \n",
    "        ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Words and their contexts\n",
    "contexts = [\n",
    "    {\n",
    "        'word': 'I',\n",
    "        'x': 3,\n",
    "        'fwd': 'START + I',\n",
    "        'bwd': 'I + love + NLP',\n",
    "        'combined': 'Full sentence'\n",
    "    },\n",
    "    {\n",
    "        'word': 'love',\n",
    "        'x': 7,\n",
    "        'fwd': 'I + love',\n",
    "        'bwd': 'love + NLP',\n",
    "        'combined': 'Full sentence'\n",
    "    },\n",
    "    {\n",
    "        'word': 'NLP',\n",
    "        'x': 11,\n",
    "        'fwd': 'I + love + NLP',\n",
    "        'bwd': 'NLP + END',\n",
    "        'combined': 'Full sentence'\n",
    "    }\n",
    "]\n",
    "\n",
    "for ctx in contexts:\n",
    "    x = ctx['x']\n",
    "    \n",
    "    # Word box\n",
    "    word_box = FancyBboxPatch((x - 0.6, 5), 1.2, 0.8,\n",
    "                              boxstyle=\"round,pad=0.1\",\n",
    "                              edgecolor='black', facecolor='#3498DB',\n",
    "                              linewidth=2.5)\n",
    "    ax.add_patch(word_box)\n",
    "    ax.text(x, 5.4, ctx['word'], ha='center', va='center',\n",
    "           fontsize=14, fontweight='bold', color='white')\n",
    "    \n",
    "    # Forward context\n",
    "    fwd_box = FancyBboxPatch((x - 1, 3.5), 2, 0.6,\n",
    "                             boxstyle=\"round,pad=0.05\",\n",
    "                             edgecolor='#5B9BD5', facecolor='#E8F4F9',\n",
    "                             linewidth=2)\n",
    "    ax.add_patch(fwd_box)\n",
    "    ax.text(x - 1, 3.9, 'Forward:', ha='left', va='center',\n",
    "           fontsize=9, fontweight='bold', color='#5B9BD5')\n",
    "    ax.text(x, 3.7, ctx['fwd'], ha='center', va='center',\n",
    "           fontsize=8, color='#2C3E50')\n",
    "    \n",
    "    # Backward context\n",
    "    bwd_box = FancyBboxPatch((x - 1, 2.5), 2, 0.6,\n",
    "                            boxstyle=\"round,pad=0.05\",\n",
    "                            edgecolor='#E67E22', facecolor='#FDF2E9',\n",
    "                            linewidth=2)\n",
    "    ax.add_patch(bwd_box)\n",
    "    ax.text(x - 1, 2.9, 'Backward:', ha='left', va='center',\n",
    "           fontsize=9, fontweight='bold', color='#E67E22')\n",
    "    ax.text(x, 2.7, ctx['bwd'], ha='center', va='center',\n",
    "           fontsize=8, color='#2C3E50')\n",
    "    \n",
    "    # Combined context\n",
    "    combined_box = FancyBboxPatch((x - 1, 1.3), 2, 0.6,\n",
    "                                 boxstyle=\"round,pad=0.05\",\n",
    "                                 edgecolor='#2ECC71', facecolor='#E8F8F5',\n",
    "                                 linewidth=2.5)\n",
    "    ax.add_patch(combined_box)\n",
    "    ax.text(x - 1, 1.7, 'BiLSTM:', ha='left', va='center',\n",
    "           fontsize=9, fontweight='bold', color='#2ECC71')\n",
    "    ax.text(x, 1.5, ctx['combined'], ha='center', va='center',\n",
    "           fontsize=8, fontweight='bold', color='#27AE60')\n",
    "\n",
    "# Legend\n",
    "ax.text(7, 0.5, '✓ BiLSTM sees ENTIRE sentence at each position (past + future)',\n",
    "        ha='center', fontsize=11, style='italic', color='#27AE60', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bilstm_context_diagram.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'bilstm_context_diagram.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: RNN vs LSTM vs BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: RNN vs LSTM vs BiLSTM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = [\n",
    "    (\"States\", \"1 (h_t)\", \"2 (h_t, C_t)\", \"2 per direction (h_fwd, C_fwd, h_bwd, C_bwd)\"),\n",
    "    (\"Directions\", \"Forward only\", \"Forward only\", \"Forward + Backward\"),\n",
    "    (\"Gates\", \"None\", \"3 gates\", \"3 gates per direction\"),\n",
    "    (\"Output dim\", \"4\", \"4\", \"8 (4 fwd + 4 bwd)\"),\n",
    "    (\"Parameters\", \"~32\", \"~128\", \"~256 (2× LSTM)\"),\n",
    "    (\"Context\", \"Past only\", \"Past only\", \"Past + Future\"),\n",
    "    (\"For 'love'\", \"Knows 'I'\", \"Knows 'I'\", \"Knows 'I' AND 'NLP'\"),\n",
    "    (\"Speed\", \"Fast\", \"Medium\", \"Slower (2 passes)\"),\n",
    "    (\"Best for\", \"Simple tasks\", \"Long sequences\", \"Tasks needing full context\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Aspect':<15} {'RNN':<20} {'LSTM':<20} {'BiLSTM':<30}\")\n",
    "print(\"-\"*80)\n",
    "for row in comparison:\n",
    "    print(f\"{row[0]:<15} {row[1]:<20} {row[2]:<20} {row[3]:<30}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nKey Advantages of BiLSTM:\")\n",
    "print(\"  1. Sees FUTURE context (backward pass)\")\n",
    "print(\"  2. Better for tasks where future info helps (NER, POS tagging, sentiment)\")\n",
    "print(\"  3. Each position has complete sentence information\")\n",
    "print()\n",
    "print(\"Trade-offs:\")\n",
    "print(\"  - 2× parameters compared to LSTM\")\n",
    "print(\"  - 2× slower (must process sequence twice)\")\n",
    "print(\"  - Cannot be used for real-time/streaming (needs full sequence)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use BiLSTM vs LSTM\n",
    "\n",
    "### Use BiLSTM when:\n",
    "- ✅ You have the **complete sequence** available\n",
    "- ✅ Future context helps (Named Entity Recognition, POS tagging)\n",
    "- ✅ Accuracy is more important than speed\n",
    "- ✅ You're doing sequence labeling (classify each token)\n",
    "\n",
    "**Examples:**\n",
    "- \"John works at **Microsoft**\" ← knowing \"Microsoft\" helps identify \"John\" as PERSON\n",
    "- \"The **bank** was steep\" vs \"The **bank** was closed\" ← future words disambiguate meaning\n",
    "\n",
    "### Use LSTM when:\n",
    "- ✅ Real-time/streaming predictions (don't have future tokens)\n",
    "- ✅ Speed is important\n",
    "- ✅ Language modeling / next word prediction\n",
    "- ✅ Sequence generation\n",
    "\n",
    "**Examples:**\n",
    "- Autocomplete (predicting next word)\n",
    "- Real-time speech recognition\n",
    "- Stock price prediction (can't see future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What is BiLSTM?\n",
    "\n",
    "**Two separate LSTMs:**\n",
    "```python\n",
    "Forward LSTM:  processes I → love → NLP\n",
    "Backward LSTM: processes NLP → love → I\n",
    "```\n",
    "\n",
    "**Output at each position:**\n",
    "```python\n",
    "BiLSTM_output[t] = [h_forward[t], h_backward[t]]\n",
    "                    ↑               ↑\n",
    "                  4 dims          4 dims\n",
    "                    └─────────┬─────────┘\n",
    "                           8 dims total\n",
    "```\n",
    "\n",
    "### Example at position \"love\":\n",
    "```\n",
    "Forward state:  encodes \"I love\" (past)\n",
    "Backward state: encodes \"love NLP\" (future)\n",
    "Combined:       encodes full sentence \"I love NLP\"\n",
    "```\n",
    "\n",
    "### Dimensions:\n",
    "- Forward hidden state: **4 dimensions**\n",
    "- Backward hidden state: **4 dimensions**\n",
    "- BiLSTM output: **8 dimensions** (concatenated)\n",
    "\n",
    "### Key Insight:\n",
    "**BiLSTM processes the sequence TWICE** (forward and backward), giving each position access to **complete context** from both past and future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
