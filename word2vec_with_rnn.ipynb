{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Word2Vec Embeddings with RNN\n",
    "\n",
    "## Overview\n",
    "This notebook shows how to use **pre-trained Word2Vec embeddings** instead of learning embeddings from scratch.\n",
    "\n",
    "### Two Approaches:\n",
    "1. **Learn embeddings** (what we did before): Start random, learn during training\n",
    "2. **Use Word2Vec** (this notebook): Start with pre-trained knowledge\n",
    "\n",
    "### Benefits of Word2Vec:\n",
    "- âœ“ Pre-trained on billions of words\n",
    "- âœ“ Already knows semantic relationships\n",
    "- âœ“ Works better with small datasets\n",
    "- âœ“ Faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Note: In real usage, you'd install gensim and load actual Word2Vec\n",
    "# pip install gensim\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Word2Vec\n",
    "\n",
    "Word2Vec creates dense vector representations where **similar words have similar vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating Word2Vec embeddings (normally you'd load pre-trained)\n",
    "# Real Word2Vec: word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors.bin')\n",
    "\n",
    "# Create simulated Word2Vec embeddings (300-dim)\n",
    "# In reality, these are pre-trained on billions of words\n",
    "embedding_dim = 300\n",
    "\n",
    "# Simulated embeddings with semantic structure\n",
    "np.random.seed(42)\n",
    "\n",
    "# Positive words cluster together\n",
    "word2vec_embeddings = {\n",
    "    'love': np.random.randn(embedding_dim) + np.array([1, 0.8, 0, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    'great': np.random.randn(embedding_dim) + np.array([0.9, 0.7, 0, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    'amazing': np.random.randn(embedding_dim) + np.array([1.1, 0.9, 0, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    'excellent': np.random.randn(embedding_dim) + np.array([0.95, 0.85, 0, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    \n",
    "    # Negative words cluster together\n",
    "    'hate': np.random.randn(embedding_dim) + np.array([-1, -0.8, 0, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    'terrible': np.random.randn(embedding_dim) + np.array([-0.9, -0.7, 0, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    'awful': np.random.randn(embedding_dim) + np.array([-1.1, -0.9, 0, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    'bad': np.random.randn(embedding_dim) + np.array([-0.8, -0.6, 0, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    \n",
    "    # Neutral/other words\n",
    "    'I': np.random.randn(embedding_dim) * 0.1,\n",
    "    'the': np.random.randn(embedding_dim) * 0.1,\n",
    "    'NLP': np.random.randn(embedding_dim) + np.array([0, 0, 1, 0, 0] + [0]*(embedding_dim-5)),\n",
    "    'movie': np.random.randn(embedding_dim) + np.array([0, 0, 0, 1, 0] + [0]*(embedding_dim-5)),\n",
    "    'bugs': np.random.randn(embedding_dim) + np.array([0, 0, 0, 0, 1] + [0]*(embedding_dim-5)),\n",
    "}\n",
    "\n",
    "print(\"Word2Vec Embedding Dimensions:\")\n",
    "print(\"=\"*60)\n",
    "for word, vec in list(word2vec_embeddings.items())[:3]:\n",
    "    print(f\"{word:10s}: 300-dim vector, first 5: {vec[:5]}\")\n",
    "print(\"...\")\n",
    "print()\n",
    "print(\"Key Property: Similar words have similar vectors!\")\n",
    "print(\"  'love' and 'great' are close in vector space\")\n",
    "print(\"  'hate' and 'terrible' are close in vector space\")\n",
    "print(\"  'love' and 'hate' are FAR apart in vector space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vocabulary and Word-to-Index Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary for our task\n",
    "# In real scenario, this comes from your training data\n",
    "\n",
    "vocab = {\n",
    "    '[PAD]': 0,   # Padding token\n",
    "    'I': 1,\n",
    "    'love': 2,\n",
    "    'hate': 3,\n",
    "    'NLP': 4,\n",
    "    'the': 5,\n",
    "    'movie': 6,\n",
    "    'great': 7,\n",
    "    'terrible': 8,\n",
    "    'bugs': 9,\n",
    "    'amazing': 10,\n",
    "    'awful': 11,\n",
    "    'excellent': 12,\n",
    "    'bad': 13,\n",
    "}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "index_to_word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(\"=\"*60)\n",
    "for word, idx in vocab.items():\n",
    "    print(f\"{word:15s} â†’ index {idx}\")\n",
    "print()\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create Embedding Matrix from Word2Vec\n",
    "\n",
    "**Key step:** Map your vocabulary to Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab, word2vec_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for Keras Embedding layer\n",
    "    \n",
    "    Args:\n",
    "        vocab: dict mapping word -> index\n",
    "        word2vec_embeddings: dict mapping word -> vector\n",
    "        embedding_dim: dimension of embeddings (300 for Word2Vec)\n",
    "    \n",
    "    Returns:\n",
    "        embedding_matrix: numpy array (vocab_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    found = 0\n",
    "    not_found = []\n",
    "    \n",
    "    for word, idx in vocab.items():\n",
    "        if word in word2vec_embeddings:\n",
    "            # Use pre-trained Word2Vec vector\n",
    "            embedding_matrix[idx] = word2vec_embeddings[word]\n",
    "            found += 1\n",
    "        else:\n",
    "            # Out-of-vocabulary: use random small vector\n",
    "            embedding_matrix[idx] = np.random.randn(embedding_dim) * 0.01\n",
    "            not_found.append(word)\n",
    "    \n",
    "    print(f\"Embedding Matrix Created:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Found in Word2Vec:     {found}/{vocab_size} words\")\n",
    "    print(f\"Not found (random):    {len(not_found)} words\")\n",
    "    if not_found:\n",
    "        print(f\"  OOV words: {not_found}\")\n",
    "    print(f\"\\nMatrix shape: {embedding_matrix.shape}\")\n",
    "    print(f\"  (vocab_size={vocab_size}, embedding_dim={embedding_dim})\")\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(vocab, word2vec_embeddings, embedding_dim)\n",
    "\n",
    "print(\"\\nExample: 'love' embedding (first 10 dimensions):\")\n",
    "print(embedding_matrix[vocab['love']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data\n",
    "training_data = [\n",
    "    (\"I love NLP\", 1),\n",
    "    (\"I hate bugs\", 0),\n",
    "    (\"the movie great\", 1),\n",
    "    (\"the movie terrible\", 0),\n",
    "    (\"I love the movie\", 1),\n",
    "    (\"I hate the movie\", 0),\n",
    "    (\"NLP amazing\", 1),\n",
    "    (\"bugs awful\", 0),\n",
    "    (\"excellent movie\", 1),\n",
    "    (\"bad movie\", 0),\n",
    "]\n",
    "\n",
    "# Tokenize and pad\n",
    "def tokenize(text, vocab):\n",
    "    return [vocab.get(word, 0) for word in text.split()]\n",
    "\n",
    "max_length = 5\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for text, label in training_data:\n",
    "    tokens = tokenize(text, vocab)\n",
    "    # Pad to max_length\n",
    "    if len(tokens) < max_length:\n",
    "        tokens = tokens + [0] * (max_length - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_length]\n",
    "    X_train.append(tokens)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(\"=\"*60)\n",
    "for i, (text, label) in enumerate(training_data):\n",
    "    sentiment = \"POSITIVE\" if label == 1 else \"NEGATIVE\"\n",
    "    print(f\"{text:20s} â†’ {sentiment:8s} | tokens: {X_train[i]}\")\n",
    "    \n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Build RNN Models - Comparison\n",
    "\n",
    "We'll create THREE models to compare:\n",
    "1. **Learned embeddings** (baseline)\n",
    "2. **Frozen Word2Vec** (trainable=False)\n",
    "3. **Fine-tuned Word2Vec** (trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    \n",
    "    # Model 1: Learn embeddings from scratch\n",
    "    print(\"\\nModel 1: LEARNED EMBEDDINGS (baseline)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model_learned = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=64,  # Smaller dimension, learned from scratch\n",
    "            input_length=max_length,\n",
    "            mask_zero=True,\n",
    "            name='learned_embedding'\n",
    "        ),\n",
    "        layers.SimpleRNN(32, return_sequences=False),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='RNN_Learned_Embeddings')\n",
    "    \n",
    "    model_learned.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Embedding: 64-dim, learned during training\")\n",
    "    print(\"âœ“ Starts: Random vectors\")\n",
    "    print(\"âœ“ After training: Task-specific embeddings\")\n",
    "    \n",
    "    # Model 2: Frozen Word2Vec\n",
    "    print(\"\\nModel 2: FROZEN WORD2VEC\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model_frozen = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_matrix],  # Pre-trained!\n",
    "            input_length=max_length,\n",
    "            trainable=False,  # FROZEN - won't update\n",
    "            mask_zero=True,\n",
    "            name='frozen_word2vec'\n",
    "        ),\n",
    "        layers.SimpleRNN(32, return_sequences=False),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='RNN_Frozen_Word2Vec')\n",
    "    \n",
    "    model_frozen.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Embedding: 300-dim, pre-trained Word2Vec\")\n",
    "    print(\"âœ“ trainable=False: Embeddings stay FIXED\")\n",
    "    print(\"âœ“ Only RNN and Dense layers train\")\n",
    "    \n",
    "    # Model 3: Fine-tuned Word2Vec\n",
    "    print(\"\\nModel 3: FINE-TUNED WORD2VEC\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model_finetuned = keras.Sequential([\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_matrix],  # Pre-trained!\n",
    "            input_length=max_length,\n",
    "            trainable=True,  # FINE-TUNE - will update\n",
    "            mask_zero=True,\n",
    "            name='finetuned_word2vec'\n",
    "        ),\n",
    "        layers.SimpleRNN(32, return_sequences=False),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name='RNN_Finetuned_Word2Vec')\n",
    "    \n",
    "    model_finetuned.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Embedding: 300-dim, pre-trained Word2Vec\")\n",
    "    print(\"âœ“ trainable=True: Embeddings CAN update\")\n",
    "    print(\"âœ“ Starts with Word2Vec, adapts to your task\")\n",
    "    print(\"âœ“ ALL layers train\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Models created successfully!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available. Showing conceptual code only.\")\n",
    "    print(\"Install with: pip install tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Compare Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"MODEL 1: Learned Embeddings\")\n",
    "    print(\"=\"*70)\n",
    "    model_learned.summary()\n",
    "    \n",
    "    print(\"\\n\\nMODEL 2: Frozen Word2Vec\")\n",
    "    print(\"=\"*70)\n",
    "    model_frozen.summary()\n",
    "    \n",
    "    print(\"\\n\\nMODEL 3: Fine-tuned Word2Vec\")\n",
    "    print(\"=\"*70)\n",
    "    model_finetuned.summary()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Key Differences:\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Learned:    {model_learned.count_params():,} total params (all trainable)\")\n",
    "    print(f\"Frozen:     {model_frozen.count_params():,} total params\")\n",
    "    print(f\"            Only {model_frozen.count_params() - vocab_size * embedding_dim:,} trainable (RNN + Dense)\")\n",
    "    print(f\"Fine-tuned: {model_finetuned.count_params():,} total params (all trainable)\")\n",
    "    \n",
    "except:\n",
    "    print(\"Models not available for summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Simulated Training Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Train all three models\n",
    "    print(\"Training Models...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    epochs = 50\n",
    "    \n",
    "    print(\"\\n1. Training Learned Embeddings...\")\n",
    "    history_learned = model_learned.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\"   âœ“ Done\")\n",
    "    \n",
    "    print(\"\\n2. Training Frozen Word2Vec...\")\n",
    "    history_frozen = model_frozen.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\"   âœ“ Done\")\n",
    "    \n",
    "    print(\"\\n3. Training Fine-tuned Word2Vec...\")\n",
    "    history_finetuned = model_finetuned.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\"   âœ“ Done\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Final Training Results:\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Learned:    Accuracy = {history_learned.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Frozen:     Accuracy = {history_frozen.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Fine-tuned: Accuracy = {history_finetuned.history['accuracy'][-1]:.4f}\")\n",
    "    \n",
    "except:\n",
    "    print(\"Training simulation not available\")\n",
    "    # Create fake history for visualization\n",
    "    history_learned = {'loss': list(np.linspace(0.7, 0.3, 50)), 'accuracy': list(np.linspace(0.5, 0.85, 50))}\n",
    "    history_frozen = {'loss': list(np.linspace(0.5, 0.2, 50)), 'accuracy': list(np.linspace(0.6, 0.95, 50))}\n",
    "    history_finetuned = {'loss': list(np.linspace(0.4, 0.15, 50)), 'accuracy': list(np.linspace(0.7, 0.98, 50))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Training Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "try:\n",
    "    # Plot 1: Loss\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(history_learned.history['loss'], label='Learned', linewidth=2, color='#E74C3C')\n",
    "    ax1.plot(history_frozen.history['loss'], label='Frozen Word2Vec', linewidth=2, color='#3498DB')\n",
    "    ax1.plot(history_finetuned.history['loss'], label='Fine-tuned Word2Vec', linewidth=2, color='#2ECC71')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(history_learned.history['accuracy'], label='Learned', linewidth=2, color='#E74C3C')\n",
    "    ax2.plot(history_frozen.history['accuracy'], label='Frozen Word2Vec', linewidth=2, color='#3498DB')\n",
    "    ax2.plot(history_finetuned.history['accuracy'], label='Fine-tuned Word2Vec', linewidth=2, color='#2ECC71')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax2.set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "except:\n",
    "    # Fallback with simulated data\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(history_learned['loss'], label='Learned', linewidth=2, color='#E74C3C')\n",
    "    ax1.plot(history_frozen['loss'], label='Frozen Word2Vec', linewidth=2, color='#3498DB')\n",
    "    ax1.plot(history_finetuned['loss'], label='Fine-tuned Word2Vec', linewidth=2, color='#2ECC71')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Comparison (Simulated)', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(history_learned['accuracy'], label='Learned', linewidth=2, color='#E74C3C')\n",
    "    ax2.plot(history_frozen['accuracy'], label='Frozen Word2Vec', linewidth=2, color='#3498DB')\n",
    "    ax2.plot(history_finetuned['accuracy'], label='Fine-tuned Word2Vec', linewidth=2, color='#2ECC71')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax2.set_title('Training Accuracy Comparison (Simulated)', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_rnn_training_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'word2vec_rnn_training_comparison.png'\")\n",
    "print(\"\\nTypical Patterns:\")\n",
    "print(\"  â€¢ Frozen Word2Vec: Often learns fastest (good initial embeddings)\")\n",
    "print(\"  â€¢ Fine-tuned: Best final performance (adapts embeddings to task)\")\n",
    "print(\"  â€¢ Learned: Slowest start (random initialization), may catch up with enough data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Visualize Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Word2Vec embeddings in 2D\n",
    "words_to_plot = ['love', 'hate', 'great', 'terrible', 'amazing', 'awful', 'excellent', 'bad']\n",
    "indices = [vocab[word] for word in words_to_plot if word in vocab]\n",
    "words = [word for word in words_to_plot if word in vocab]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings_to_plot = embedding_matrix[indices]\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings_to_plot)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Color by sentiment\n",
    "positive_words = ['love', 'great', 'amazing', 'excellent']\n",
    "negative_words = ['hate', 'terrible', 'awful', 'bad']\n",
    "\n",
    "colors = ['green' if word in positive_words else 'red' for word in words]\n",
    "\n",
    "ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "          c=colors, s=200, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "               fontsize=12, fontweight='bold', ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('PCA Component 1', fontsize=12)\n",
    "ax.set_ylabel('PCA Component 2', fontsize=12)\n",
    "ax.set_title('Word2Vec Embedding Space (2D Projection)\\nGreen=Positive, Red=Negative', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', label='Positive words'),\n",
    "    Patch(facecolor='red', label='Negative words')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='best', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_embedding_space.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'word2vec_embedding_space.png'\")\n",
    "print(\"\\nNotice:\")\n",
    "print(\"  â€¢ Positive words cluster together (green)\")\n",
    "print(\"  â€¢ Negative words cluster together (red)\")\n",
    "print(\"  â€¢ This semantic structure helps the RNN learn faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Real-World Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real-World Code Example: Loading Google's Word2Vec\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "# Step 1: Install gensim\n",
    "pip install gensim\n",
    "\n",
    "# Step 2: Download Google's pre-trained Word2Vec\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
    "# GoogleNews-vectors-negative300.bin.gz (~1.5 GB)\n",
    "\n",
    "# Step 3: Load Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz',\n",
    "    binary=True\n",
    ")\n",
    "\n",
    "# Step 4: Explore\n",
    "print(word2vec['king'])  # 300-dim vector\n",
    "print(word2vec.most_similar('king'))  # Similar words\n",
    "# [('queen', 0.65), ('monarch', 0.58), ...]\n",
    "\n",
    "# Step 5: Create embedding matrix (same as we did)\n",
    "embedding_matrix = create_embedding_matrix(vocab, word2vec, 300)\n",
    "\n",
    "# Step 6: Use in Keras\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=300,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=False  # or True for fine-tuning\n",
    "    ),\n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Decision Guide - When to Use What?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Guide: Which Approach to Use?\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"1. LEARNED EMBEDDINGS (from scratch)\")\n",
    "print(\"-\"*70)\n",
    "print(\"âœ“ Use when:\")\n",
    "print(\"  â€¢ Large dataset (100K+ examples)\")\n",
    "print(\"  â€¢ Very domain-specific vocabulary (medical, legal)\")\n",
    "print(\"  â€¢ Word meanings differ from general use\")\n",
    "print(\"  â€¢ Have GPU and training time\")\n",
    "print()\n",
    "print(\"âœ— Avoid when:\")\n",
    "print(\"  â€¢ Small dataset (<10K examples)\")\n",
    "print(\"  â€¢ General domain text\")\n",
    "print(\"  â€¢ Limited resources\")\n",
    "print()\n",
    "\n",
    "print(\"2. FROZEN WORD2VEC (trainable=False)\")\n",
    "print(\"-\"*70)\n",
    "print(\"âœ“ Use when:\")\n",
    "print(\"  â€¢ Small dataset (1K-10K examples)\")\n",
    "print(\"  â€¢ General domain (news, reviews, social media)\")\n",
    "print(\"  â€¢ Want fast training\")\n",
    "print(\"  â€¢ Limited compute resources\")\n",
    "print(\"  â€¢ Quick baseline/prototype\")\n",
    "print()\n",
    "print(\"âœ— Avoid when:\")\n",
    "print(\"  â€¢ Very domain-specific text\")\n",
    "print(\"  â€¢ Many out-of-vocabulary words\")\n",
    "print()\n",
    "\n",
    "print(\"3. FINE-TUNED WORD2VEC (trainable=True)\")\n",
    "print(\"-\"*70)\n",
    "print(\"âœ“ Use when:\")\n",
    "print(\"  â€¢ Medium dataset (10K-100K examples)\")\n",
    "print(\"  â€¢ Want best accuracy\")\n",
    "print(\"  â€¢ Have some compute resources\")\n",
    "print(\"  â€¢ Domain is somewhat specialized\")\n",
    "print(\"  â€¢ BEST OF BOTH WORLDS approach\")\n",
    "print()\n",
    "print(\"âœ— Avoid when:\")\n",
    "print(\"  â€¢ Very small dataset (might overfit)\")\n",
    "print(\"  â€¢ Very limited resources\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRECOMMENDED WORKFLOW:\")\n",
    "print(\"  1. Start with FROZEN Word2Vec (quick baseline)\")\n",
    "print(\"  2. If accuracy is good â†’ Done! ðŸŽ‰\")\n",
    "print(\"  3. If accuracy is poor â†’ Try FINE-TUNING\")\n",
    "print(\"  4. If still poor â†’ Consider learning from scratch or more data\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "**1. Word2Vec gives RNN a \"head start\"**\n",
    "- Pre-trained on billions of words\n",
    "- Already knows \"love\" â‰ˆ \"great\" and \"hate\" â‰ˆ \"terrible\"\n",
    "- Faster training, better results with small data\n",
    "\n",
    "**2. Three approaches:**\n",
    "```python\n",
    "# Learned (from scratch)\n",
    "Embedding(vocab_size, 64)  # Random â†’ task-specific\n",
    "\n",
    "# Frozen (fixed)\n",
    "Embedding(vocab_size, 300, weights=[word2vec], trainable=False)\n",
    "\n",
    "# Fine-tuned (adaptable)\n",
    "Embedding(vocab_size, 300, weights=[word2vec], trainable=True)\n",
    "```\n",
    "\n",
    "**3. How to create embedding matrix:**\n",
    "```python\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, idx in vocab.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = random_vector\n",
    "```\n",
    "\n",
    "**4. Real usage:**\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews.bin', binary=True)\n",
    "# Then use in Keras as shown above\n",
    "```\n",
    "\n",
    "**5. Best practice:**\n",
    "- Start with frozen Word2Vec (quick baseline)\n",
    "- Fine-tune if needed for better accuracy\n",
    "- Only train from scratch if you have lots of data\n",
    "\n",
    "### Other Pre-trained Options:\n",
    "- **GloVe** (Stanford): Similar to Word2Vec\n",
    "- **FastText** (Facebook): Handles out-of-vocabulary better\n",
    "- **BERT embeddings**: Contextual (more advanced)\n",
    "\n",
    "### The Big Picture:\n",
    "**Word2Vec + RNN** combines the best of both:\n",
    "- Word2Vec: Rich semantic knowledge\n",
    "- RNN: Sequential pattern learning\n",
    "- Result: Better performance, especially with limited data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
