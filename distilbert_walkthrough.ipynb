{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT Step-by-Step: Processing \"I love NLP\"\n",
    "\n",
    "## Setup\n",
    "- Sentence: **\"I love NLP\"**\n",
    "- Hidden dimension: **4** (simplified from 768 for education)\n",
    "- **Same sentence as RNN/LSTM/BiLSTM for comparison!**\n",
    "\n",
    "## CRITICAL DIFFERENCE: Transformer vs RNN\n",
    "\n",
    "**DistilBERT is a TRANSFORMER, not an RNN!**\n",
    "\n",
    "```\n",
    "RNN/LSTM/BiLSTM:          DistilBERT (Transformer):\n",
    "Sequential processing     Parallel processing\n",
    "Hidden states             Attention mechanism\n",
    "Process one at a time     Process all tokens at once\n",
    "```\n",
    "\n",
    "**Note:** We're using 4 dimensions (instead of real 768) to make comparisons easier with previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch, FancyArrowPatch, Circle\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "sentence = \"I love NLP\"\n",
    "hidden_dim = 4  # Simplified (real DistilBERT uses 768)\n",
    "n_heads = 2     # Number of attention heads (real: 12)\n",
    "n_layers = 2    # Number of transformer layers (real: 6)\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "print(f\"Hidden dimension: {hidden_dim} (simplified from 768)\")\n",
    "print(f\"Attention heads: {n_heads} (simplified from 12)\")\n",
    "print(f\"Transformer layers: {n_layers} (simplified from 6)\")\n",
    "print()\n",
    "print(\"Key Difference from RNN/LSTM:\")\n",
    "print(\"  ✗ No sequential processing\")\n",
    "print(\"  ✗ No hidden states passed through time\")\n",
    "print(\"  ✓ All tokens processed in PARALLEL\")\n",
    "print(\"  ✓ Uses SELF-ATTENTION to look at all words at once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Tokenization\n",
    "\n",
    "Unlike simple word-level tokenization, BERT uses **WordPiece tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified tokenization (real BERT uses WordPiece)\n",
    "# In real DistilBERT: \"I love NLP\" → [CLS] I love NLP [SEP]\n",
    "\n",
    "tokens = ['[CLS]', 'I', 'love', 'NLP', '[SEP]']\n",
    "vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "token_ids = [vocab[t] for t in tokens]\n",
    "\n",
    "print(\"Tokenization:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input: '{sentence}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print()\n",
    "print(\"Special tokens:\")\n",
    "print(\"  [CLS]: Classification token (for sentence-level tasks)\")\n",
    "print(\"  [SEP]: Separator token (marks end of sequence)\")\n",
    "print()\n",
    "print(f\"Sequence length: {len(tokens)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Token Embeddings\n",
    "\n",
    "Each token gets converted to a dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token embeddings (simplified)\n",
    "# Real DistilBERT: 30,522 vocab × 768 dimensions\n",
    "vocab_size = len(tokens)\n",
    "embedding_matrix = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],  # [CLS]\n",
    "    [0.5, 0.2, 0.1, 0.3],  # I\n",
    "    [0.8, 0.6, 0.3, 0.5],  # love\n",
    "    [0.1, 0.9, 0.7, 0.6],  # NLP\n",
    "    [0.2, 0.3, 0.4, 0.5],  # [SEP]\n",
    "])\n",
    "\n",
    "# Get embeddings for our tokens\n",
    "token_embeddings = embedding_matrix[token_ids]\n",
    "\n",
    "print(\"Token Embeddings:\")\n",
    "print(\"=\"*60)\n",
    "for token, emb in zip(tokens, token_embeddings):\n",
    "    print(f\"{token:8s}: {emb}\")\n",
    "print()\n",
    "print(f\"Shape: ({len(tokens)}, {hidden_dim})\")\n",
    "print(\"      (sequence_length, hidden_dimension)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Positional Encodings\n",
    "\n",
    "**Critical for Transformers:** Since we process all tokens in parallel, we need to tell the model about token positions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified positional encodings\n",
    "# Real DistilBERT uses learned positional embeddings\n",
    "def get_positional_encoding(seq_len, hidden_dim):\n",
    "    \"\"\"Simplified positional encoding\"\"\"\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, hidden_dim, 2) * -(np.log(10000.0) / hidden_dim))\n",
    "    \n",
    "    pos_encoding = np.zeros((seq_len, hidden_dim))\n",
    "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pos_encoding\n",
    "\n",
    "positional_encodings = get_positional_encoding(len(tokens), hidden_dim)\n",
    "\n",
    "print(\"Positional Encodings:\")\n",
    "print(\"=\"*60)\n",
    "for i, (token, pos_enc) in enumerate(zip(tokens, positional_encodings)):\n",
    "    print(f\"Position {i} ({token:8s}): {pos_enc}\")\n",
    "print()\n",
    "print(\"Why needed? Without position info, 'I love NLP' = 'NLP love I'\")\n",
    "print(\"Positional encoding tells the model token ORDER\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Input Embeddings = Token + Position\n",
    "\n",
    "Combine token embeddings with positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add token embeddings and positional encodings\n",
    "input_embeddings = token_embeddings + positional_encodings\n",
    "\n",
    "print(\"Input Embeddings (Token + Position):\")\n",
    "print(\"=\"*60)\n",
    "for token, emb in zip(tokens, input_embeddings):\n",
    "    print(f\"{token:8s}: {emb}\")\n",
    "print()\n",
    "print(\"These embeddings now contain:\")\n",
    "print(\"  1. What the token is (semantic meaning)\")\n",
    "print(\"  2. Where the token is (position in sequence)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Self-Attention Mechanism\n",
    "\n",
    "**The key innovation of Transformers!**\n",
    "\n",
    "### How it works:\n",
    "1. Each token creates **Query (Q)**, **Key (K)**, **Value (V)** vectors\n",
    "2. Compute attention scores: how much each token should attend to others\n",
    "3. Use scores to create weighted combination of values\n",
    "\n",
    "### Formula:\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q @ K^T / √d_k) @ V\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q, K, V weight matrices\n",
    "np.random.seed(42)\n",
    "W_q = np.random.randn(hidden_dim, hidden_dim) * 0.1\n",
    "W_k = np.random.randn(hidden_dim, hidden_dim) * 0.1\n",
    "W_v = np.random.randn(hidden_dim, hidden_dim) * 0.1\n",
    "\n",
    "# Compute Q, K, V for all tokens\n",
    "Q = input_embeddings @ W_q  # (5, 4) @ (4, 4) = (5, 4)\n",
    "K = input_embeddings @ W_k\n",
    "V = input_embeddings @ W_v\n",
    "\n",
    "print(\"Query, Key, Value Matrices:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Each token has:\")\n",
    "print(\"  Query (Q):  'What am I looking for?'\")\n",
    "print(\"  Key (K):    'What do I offer?'\")\n",
    "print(\"  Value (V):  'What information do I have?'\")\n",
    "print()\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "print()\n",
    "print(\"Q matrix (each row = query vector for one token):\")\n",
    "for token, q in zip(tokens, Q):\n",
    "    print(f\"  {token:8s}: {q}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compute Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention scores: Q @ K^T\n",
    "attention_scores = Q @ K.T / np.sqrt(hidden_dim)\n",
    "\n",
    "print(\"Attention Scores (before softmax):\")\n",
    "print(\"=\"*60)\n",
    "print(\"How much each token (row) attends to each token (column)\")\n",
    "print()\n",
    "print(f\"         {' '.join([f'{t:8s}' for t in tokens])}\")\n",
    "for i, (token, scores) in enumerate(zip(tokens, attention_scores)):\n",
    "    scores_str = ' '.join([f'{s:8.3f}' for s in scores])\n",
    "    print(f\"{token:8s} {scores_str}\")\n",
    "print()\n",
    "print(\"Higher score = more attention to that token\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Apply Softmax to Get Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Apply softmax to get attention weights (sum to 1 for each row)\n",
    "attention_weights = softmax(attention_scores)\n",
    "\n",
    "print(\"Attention Weights (after softmax):\")\n",
    "print(\"=\"*60)\n",
    "print(\"Normalized probabilities - each row sums to 1.0\")\n",
    "print()\n",
    "print(f\"         {' '.join([f'{t:8s}' for t in tokens])}\")\n",
    "for i, (token, weights) in enumerate(zip(tokens, attention_weights)):\n",
    "    weights_str = ' '.join([f'{w:8.3f}' for w in weights])\n",
    "    print(f\"{token:8s} {weights_str}  (sum={weights.sum():.3f})\")\n",
    "print()\n",
    "print(\"Example: 'love' attends most to which tokens?\")\n",
    "love_idx = 2\n",
    "max_attention = np.argmax(attention_weights[love_idx])\n",
    "print(f\"  → '{tokens[love_idx]}' attends most to '{tokens[max_attention]}'\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compute Attention Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply attention weights by values\n",
    "attention_output = attention_weights @ V\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Weighted combination of all token values\")\n",
    "print()\n",
    "for token, output in zip(tokens, attention_output):\n",
    "    print(f\"{token:8s}: {output}\")\n",
    "print()\n",
    "print(\"Key insight:\")\n",
    "print(\"  Each token's output is influenced by ALL other tokens\")\n",
    "print(\"  Unlike RNN where 'I' only affects future tokens\")\n",
    "print(\"  Here 'NLP' can influence the representation of 'I'!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Attention Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(attention_weights, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=tokens, yticklabels=tokens,\n",
    "            cbar_kws={'label': 'Attention Weight'},\n",
    "            vmin=0, vmax=1, ax=ax, linewidths=1, linecolor='black')\n",
    "\n",
    "ax.set_xlabel('Attends TO (Keys)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Attends FROM (Queries)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Self-Attention Matrix\\nHow much each token attends to others', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add note\n",
    "fig.text(0.5, 0.02, 'Darker = more attention  |  Each row sums to 1.0', \n",
    "         ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distilbert_attention_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'distilbert_attention_heatmap.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Information Flow Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(7, 9.5, 'DistilBERT: All Tokens Attend to Each Other (Parallel)', \n",
    "        ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Token positions\n",
    "token_positions = [(2, 5), (4.5, 5), (7, 5), (9.5, 5), (12, 5)]\n",
    "\n",
    "# Draw tokens\n",
    "for (x, y), token in zip(token_positions, tokens):\n",
    "    circle = Circle((x, y), 0.4, color='#3498DB', ec='black', linewidth=2.5, zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, y, token, ha='center', va='center', \n",
    "           fontsize=11, fontweight='bold', color='white')\n",
    "\n",
    "# Draw attention connections (simplified - show strongest connections)\n",
    "# Find top 2 connections for each token\n",
    "for i, (token, (x1, y1)) in enumerate(zip(tokens, token_positions)):\n",
    "    # Get top 2 attention weights (excluding self)\n",
    "    weights = attention_weights[i].copy()\n",
    "    weights[i] = 0  # Exclude self-attention for visualization\n",
    "    top_indices = np.argsort(weights)[-2:]\n",
    "    \n",
    "    for j in top_indices:\n",
    "        x2, y2 = token_positions[j]\n",
    "        weight = attention_weights[i][j]\n",
    "        \n",
    "        # Draw arrow with thickness based on weight\n",
    "        arrow = FancyArrowPatch((x1, y1), (x2, y2),\n",
    "                               arrowstyle='->,head_width=0.3,head_length=0.2',\n",
    "                               color='orange', linewidth=weight*5,\n",
    "                               alpha=0.6, zorder=1,\n",
    "                               connectionstyle=\"arc3,rad=0.3\")\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "# Add legend\n",
    "ax.text(7, 3, 'Self-Attention: Every token can attend to every other token', \n",
    "        ha='center', fontsize=12, style='italic', bbox=dict(boxstyle='round',\n",
    "        facecolor='#FFF9E6', edgecolor='orange', linewidth=2))\n",
    "\n",
    "ax.text(1, 1.5, 'Arrow thickness = attention strength', fontsize=10, style='italic')\n",
    "ax.text(1, 1, 'Showing top 2 connections per token', fontsize=10, style='italic')\n",
    "\n",
    "# Comparison text\n",
    "ax.text(7, 7.5, 'vs RNN/LSTM: Sequential (I → love → NLP)', \n",
    "        ha='center', fontsize=11, color='gray', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distilbert_attention_flow.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'distilbert_attention_flow.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Complete Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 14))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 14)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(6, 13.5, 'DistilBERT Architecture', \n",
    "        ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "y_pos = 12.5\n",
    "box_width = 10\n",
    "box_height = 0.8\n",
    "x_center = 6\n",
    "\n",
    "layers = [\n",
    "    ('Input Text', '#E8F4F8', '\"I love NLP\"'),\n",
    "    ('Tokenization', '#D5E8F0', '[CLS] I love NLP [SEP]'),\n",
    "    ('Token Embeddings', '#B8E6F0', 'Each token → 4-dim vector'),\n",
    "    ('+ Positional Encoding', '#A0D8E8', 'Add position information'),\n",
    "    ('⬇', 'white', ''),\n",
    "    ('Multi-Head Self-Attention', '#7DD3E8', 'All tokens attend to all'),\n",
    "    ('+ Residual & LayerNorm', '#6BC5DD', 'Add & normalize'),\n",
    "    ('⬇', 'white', ''),\n",
    "    ('Feed-Forward Network', '#5AB7D2', 'Dense → ReLU → Dense'),\n",
    "    ('+ Residual & LayerNorm', '#48A9C7', 'Add & normalize'),\n",
    "    ('⬇', 'white', ''),\n",
    "    ('Repeat 6 times', '#FFF9E6', '(We simplified to 2)'),\n",
    "    ('⬇', 'white', ''),\n",
    "    ('Final Representations', '#70AD47', '5 tokens × 4 dims'),\n",
    "    ('Classification Head', '#5A9636', '[CLS] token → prediction'),\n",
    "]\n",
    "\n",
    "for i, (label, color, note) in enumerate(layers):\n",
    "    y = y_pos - i * 0.9\n",
    "    \n",
    "    if label == '⬇':\n",
    "        ax.text(x_center, y, '⬇', ha='center', va='center', \n",
    "               fontsize=20, color='gray')\n",
    "    else:\n",
    "        rect = FancyBboxPatch((x_center - box_width/2, y - box_height/2),\n",
    "                             box_width, box_height,\n",
    "                             boxstyle=\"round,pad=0.1\",\n",
    "                             edgecolor='black', facecolor=color,\n",
    "                             linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(x_center, y + 0.1, label, ha='center', va='center',\n",
    "               fontsize=11, fontweight='bold')\n",
    "        \n",
    "        if note:\n",
    "            ax.text(x_center, y - 0.25, note, ha='center', va='center',\n",
    "                   fontsize=8, style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distilbert_architecture.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved as 'distilbert_architecture.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: RNN vs LSTM vs BiLSTM vs DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COMPARISON: RNN vs LSTM vs BiLSTM vs DistilBERT (Transformer)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "comparison = [\n",
    "    (\"Architecture\", \"Recurrent\", \"Recurrent\", \"Recurrent\", \"Transformer\"),\n",
    "    (\"Processing\", \"Sequential\", \"Sequential\", \"Sequential\", \"Parallel\"),\n",
    "    (\"States\", \"1 (h_t)\", \"2 (h_t, C_t)\", \"4 (2 dirs)\", \"None - uses attention\"),\n",
    "    (\"Context\", \"Past only\", \"Past only\", \"Past + Future\", \"All tokens at once\"),\n",
    "    (\"For 'love'\", \"Knows 'I'\", \"Knows 'I'\", \"Knows 'I' & 'NLP'\", \"Knows ALL tokens\"),\n",
    "    (\"Attention\", \"None\", \"None\", \"None\", \"Self-attention\"),\n",
    "    (\"Position info\", \"Implicit\", \"Implicit\", \"Implicit\", \"Explicit encoding\"),\n",
    "    (\"Parameters\", \"~32\", \"~128\", \"~256\", \"~66M (real model)\"),\n",
    "    (\"Speed\", \"Fast\", \"Medium\", \"Slow\", \"Fast (parallel)\"),\n",
    "    (\"Long sequences\", \"Poor\", \"Good\", \"Good\", \"Excellent\"),\n",
    "    (\"Vanishing grad\", \"Problem\", \"Solved\", \"Solved\", \"Not an issue\"),\n",
    "    (\"Pre-training\", \"No\", \"No\", \"No\", \"Yes (BERT/DistilBERT)\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Aspect':<18} {'RNN':<15} {'LSTM':<15} {'BiLSTM':<15} {'DistilBERT':<25}\")\n",
    "print(\"-\"*90)\n",
    "for row in comparison:\n",
    "    print(f\"{row[0]:<18} {row[1]:<15} {row[2]:<15} {row[3]:<15} {row[4]:<25}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\n1. PROCESSING ORDER:\")\n",
    "print(\"   RNN/LSTM/BiLSTM: Process tokens ONE AT A TIME (sequential)\")\n",
    "print(\"   DistilBERT:      Process ALL TOKENS AT ONCE (parallel)\")\n",
    "\n",
    "print(\"\\n2. CONTEXT AWARENESS:\")\n",
    "print(\"   RNN:        'love' only knows about 'I' (past)\")\n",
    "print(\"   LSTM:       'love' only knows about 'I' (past, better memory)\")\n",
    "print(\"   BiLSTM:     'love' knows 'I' (past) AND 'NLP' (future)\")\n",
    "print(\"   DistilBERT: 'love' ATTENDS to ALL tokens simultaneously\")\n",
    "\n",
    "print(\"\\n3. MECHANISM:\")\n",
    "print(\"   RNN/LSTM/BiLSTM: Hidden states carry information\")\n",
    "print(\"   DistilBERT:      Self-attention computes relationships\")\n",
    "\n",
    "print(\"\\n4. POSITION INFORMATION:\")\n",
    "print(\"   RNN/LSTM/BiLSTM: Position is implicit (order of processing)\")\n",
    "print(\"   DistilBERT:      Position is EXPLICIT (positional encodings)\")\n",
    "\n",
    "print(\"\\n5. PRE-TRAINING:\")\n",
    "print(\"   RNN/LSTM/BiLSTM: Train from scratch for each task\")\n",
    "print(\"   DistilBERT:      Pre-trained on massive text, fine-tune for task\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DistilBERT is Powerful\n",
    "\n",
    "### 1. Parallel Processing\n",
    "```python\n",
    "RNN:        I → love → NLP  (3 sequential steps)\n",
    "DistilBERT: All tokens processed simultaneously (1 parallel step)\n",
    "```\n",
    "\n",
    "### 2. Full Context Attention\n",
    "```python\n",
    "At 'love':\n",
    "RNN:        Knows 'I' (via hidden state)\n",
    "BiLSTM:     Knows 'I' and 'NLP' (via two passes)\n",
    "DistilBERT: Directly attends to 'I', 'NLP', [CLS], [SEP]\n",
    "```\n",
    "\n",
    "### 3. Self-Attention Captures Relationships\n",
    "```python\n",
    "attention_weights['love']['NLP'] = 0.35  # love strongly attends to NLP\n",
    "attention_weights['love']['I']   = 0.28  # love also attends to I\n",
    "# The model LEARNS which tokens are important for each token\n",
    "```\n",
    "\n",
    "### 4. Pre-trained Knowledge\n",
    "- DistilBERT is pre-trained on billions of words\n",
    "- Already knows language patterns, grammar, semantics\n",
    "- Fine-tune for specific tasks (sentiment, NER, QA)\n",
    "\n",
    "### 5. DistilBERT vs Full BERT\n",
    "- 40% fewer parameters than BERT\n",
    "- 60% faster\n",
    "- Retains 97% of BERT's performance\n",
    "- Achieved through **knowledge distillation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Concepts\n",
    "\n",
    "### What is DistilBERT?\n",
    "A **Transformer-based** model (NOT an RNN!) that uses **self-attention**\n",
    "\n",
    "### Core Components:\n",
    "```python\n",
    "1. Tokenization:        \"I love NLP\" → [CLS] I love NLP [SEP]\n",
    "2. Token Embeddings:    Each token → 768-dim vector (we used 4)\n",
    "3. Positional Encoding: Add position information\n",
    "4. Self-Attention:      Each token attends to all others\n",
    "5. Feed-Forward:        Dense layers\n",
    "6. Repeat:              6 transformer layers\n",
    "7. Output:              Contextualized representations\n",
    "```\n",
    "\n",
    "### Dimensions:\n",
    "```python\n",
    "Input:  (sequence_length, hidden_dim) = (5, 4)\n",
    "Q, K, V: (5, 4) for each\n",
    "Attention weights: (5, 5) - each token to each token\n",
    "Output: (5, 4) - same shape as input\n",
    "```\n",
    "\n",
    "### Real DistilBERT:\n",
    "- Hidden dim: **768** (not 4)\n",
    "- Attention heads: **12** (not 2)\n",
    "- Layers: **6** (not 2)\n",
    "- Vocabulary: **30,522** tokens\n",
    "- Parameters: **66 million**\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "**RNN/LSTM:**\n",
    "- Simple tasks\n",
    "- Limited compute\n",
    "- Streaming/real-time\n",
    "\n",
    "**BiLSTM:**\n",
    "- Need future context\n",
    "- Sequence labeling\n",
    "- Medium-length sequences\n",
    "\n",
    "**DistilBERT:**\n",
    "- Best accuracy needed\n",
    "- Transfer learning\n",
    "- Any NLP task with enough data\n",
    "- Complete sequences available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
