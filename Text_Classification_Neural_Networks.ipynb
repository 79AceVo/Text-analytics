{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BBC News Text Classification with Neural Networks\n",
        "\n",
        "Multi-class text classification using the BBC News dataset with 5 categories:\n",
        "- Business (510 articles)\n",
        "- Entertainment (386 articles)\n",
        "- Politics (417 articles)\n",
        "- Sport (511 articles)\n",
        "- Tech (401 articles)\n",
        "\n",
        "We will use Neural Networks (Traditional, CNN, RNN, LSTM) to train and predict."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn import metrics\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import string\n",
        "import re"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load BBC News Dataset\n",
        "\n",
        "The BBC dataset contains 2,225 news articles in 5 categories.\n",
        "\n",
        "**File format:** Tab-separated (\\t) with columns: category, filename, title, content"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the bbc-news-data.csv file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload bbc-news-data.csv"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the tab-separated CSV file\n",
        "df = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "df.head()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(df['category'].value_counts())\n",
        "print(\"\\nPercentage:\")\n",
        "print((df['category'].value_counts(normalize=True) * 100).round(2))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize class distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
        "df['category'].value_counts().plot(kind='bar', color=colors)\n",
        "plt.title('BBC News Category Distribution')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label encoding\n",
        "label_mapping = {label: idx for idx, label in enumerate(sorted(df['category'].unique()))}\n",
        "reverse_mapping = {idx: label for label, idx in label_mapping.items()}\n",
        "\n",
        "print(\"Label Mapping:\")\n",
        "for label, idx in label_mapping.items():\n",
        "    print(f\"  {idx}: {label}\")\n",
        "\n",
        "df['label'] = df['category'].map(label_mapping)\n",
        "num_classes = len(label_mapping)\n",
        "print(f\"\\nNumber of classes: {num_classes}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text from each category\n",
        "for category in df['category'].unique():\n",
        "    sample = df[df['category'] == category]['content'].iloc[0][:300]\n",
        "    title = df[df['category'] == category]['title'].iloc[0]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{category.upper()}\")\n",
        "    print(f\"Title: {title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"{sample}...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to content column\n",
        "df['clean_text'] = df['content'].apply(clean_text)\n",
        "df[['category', 'title', 'clean_text']].head()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check text length distribution\n",
        "df['text_length'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "print(\"Text Length Statistics (words):\")\n",
        "print(df['text_length'].describe())\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.title('Distribution of Text Length (words)')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(df['text_length'].mean(), color='red', linestyle='--', label=f\"Mean: {df['text_length'].mean():.0f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling\n",
        "\n",
        "Steps:\n",
        "1. Split data into train and test (75/25)\n",
        "2. Vectorize text using CountVectorizer\n",
        "3. Train neural network models\n",
        "4. Evaluate performance"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Train-test split\n",
        "X = df['clean_text']\n",
        "y = df['label']\n",
        "\n",
        "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "\n",
        "# Stratified split to maintain class distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Check distribution in splits\n",
        "print(f\"\\nTraining class distribution:\")\n",
        "for idx in sorted(y_train.unique()):\n",
        "    count = sum(y_train == idx)\n",
        "    print(f\"  {reverse_mapping[idx]}: {count}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2-3: Vectorize text\n",
        "vect = CountVectorizer(max_features=10000, preprocessor=clean_text)\n",
        "X_train_dtm = vect.fit_transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vect.vocabulary_)}\")\n",
        "print(f\"Training DTM shape: {X_train_dtm.shape}\")\n",
        "print(f\"Test DTM shape: {X_test_dtm.shape}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Setup\n",
        "\n",
        "**GPU Setup:** Runtime > Change runtime type > T4 GPU"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences for neural network input\n",
        "max_len = 5000  # Maximum sequence length\n",
        "\n",
        "X_train_dense = pad_sequences(X_train_dtm.toarray(), maxlen=max_len, padding='post', truncating='post')\n",
        "X_test_dense = pad_sequences(X_test_dtm.toarray(), maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "print(f\"X_train_dense shape: {X_train_dense.shape}\")\n",
        "print(f\"X_test_dense shape: {X_test_dense.shape}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to categorical (one-hot encoding) for multi-class\n",
        "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "print(f\"y_train_cat shape: {y_train_cat.shape}\")\n",
        "print(f\"y_test_cat shape: {y_test_cat.shape}\")\n",
        "print(f\"\\nSample one-hot label: {y_train_cat[0]} -> {reverse_mapping[np.argmax(y_train_cat[0])]}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Classic Neural Network (Dense Layers)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model for multi-class classification\n",
        "model_dense = keras.Sequential([\n",
        "    keras.layers.Embedding(input_dim=X_train_dense.shape[1], output_dim=64),\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class\n",
        "])\n",
        "\n",
        "# Compile with categorical crossentropy for multi-class\n",
        "model_dense.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_dense.summary()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history_dense = model_dense.fit(\n",
        "    X_train_dense, y_train_cat,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_dense, y_test_cat),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history_dense.history['accuracy'], label='Train')\n",
        "axes[0].plot(history_dense.history['val_accuracy'], label='Validation')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history_dense.history['loss'], label='Train')\n",
        "axes[1].plot(history_dense.history['val_loss'], label='Validation')\n",
        "axes[1].set_title('Model Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model 1\n",
        "loss, accuracy = model_dense.evaluate(X_test_dense, y_test_cat)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Get predictions\n",
        "predictions_dense = model_dense.predict(X_test_dense)\n",
        "predicted_classes_dense = np.argmax(predictions_dense, axis=1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Model 1: Dense Neural Network\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(y_test, predicted_classes_dense, target_names=sorted(label_mapping.keys())))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_classes_dense)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=sorted(label_mapping.keys()),\n",
        "            yticklabels=sorted(label_mapping.keys()))\n",
        "plt.title('Confusion Matrix - Dense NN')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: CNN (Convolutional Neural Network)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model for text classification\n",
        "model_cnn = keras.Sequential([\n",
        "    keras.layers.Embedding(input_dim=X_train_dense.shape[1], output_dim=128),\n",
        "    keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    keras.layers.GlobalMaxPooling1D(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_cnn.summary()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CNN\n",
        "history_cnn = model_cnn.fit(\n",
        "    X_train_dense, y_train_cat,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_dense, y_test_cat),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate CNN\n",
        "predictions_cnn = model_cnn.predict(X_test_dense)\n",
        "predicted_classes_cnn = np.argmax(predictions_cnn, axis=1)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Model 2: CNN\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(y_test, predicted_classes_cnn, target_names=sorted(label_mapping.keys())))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: LSTM (Long Short-Term Memory)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "model_lstm = keras.Sequential([\n",
        "    keras.layers.Embedding(input_dim=X_train_dense.shape[1], output_dim=128),\n",
        "    keras.layers.LSTM(64, return_sequences=False),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_lstm.summary()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LSTM\n",
        "history_lstm = model_lstm.fit(\n",
        "    X_train_dense, y_train_cat,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_dense, y_test_cat),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate LSTM\n",
        "predictions_lstm = model_lstm.predict(X_test_dense)\n",
        "predicted_classes_lstm = np.argmax(predictions_lstm, axis=1)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Model 3: LSTM\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(y_test, predicted_classes_lstm, target_names=sorted(label_mapping.keys())))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: Bidirectional LSTM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional LSTM Model\n",
        "model_bilstm = keras.Sequential([\n",
        "    keras.layers.Embedding(input_dim=X_train_dense.shape[1], output_dim=128),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_bilstm.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_bilstm.summary()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Bidirectional LSTM\n",
        "history_bilstm = model_bilstm.fit(\n",
        "    X_train_dense, y_train_cat,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_dense, y_test_cat),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Bidirectional LSTM\n",
        "predictions_bilstm = model_bilstm.predict(X_test_dense)\n",
        "predicted_classes_bilstm = np.argmax(predictions_bilstm, axis=1)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Model 4: Bidirectional LSTM\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(y_test, predicted_classes_bilstm, target_names=sorted(label_mapping.keys())))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare all models\n",
        "models = ['Dense NN', 'CNN', 'LSTM', 'Bi-LSTM']\n",
        "predictions_all = [predicted_classes_dense, predicted_classes_cnn, predicted_classes_lstm, predicted_classes_bilstm]\n",
        "\n",
        "accuracies = [accuracy_score(y_test, pred) for pred in predictions_all]\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': accuracies\n",
        "}).sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
        "bars = plt.bar(models, accuracies, color=colors)\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "| Model | Best For |\n",
        "|-------|----------|\n",
        "| **Dense NN** | Fast training, baseline performance |\n",
        "| **CNN** | Capturing local patterns in text |\n",
        "| **LSTM** | Sequential dependencies |\n",
        "| **Bi-LSTM** | Context from both directions |\n",
        "\n",
        "### Multi-class Classification Notes:\n",
        "- Use `softmax` activation (not sigmoid) for output layer\n",
        "- Use `categorical_crossentropy` loss (not binary)\n",
        "- Convert labels to one-hot encoding with `to_categorical()`\n",
        "- Use `np.argmax()` to get predicted classes from probabilities\n",
        "- Evaluate with macro F1 for balanced assessment across classes\n",
        "\n",
        "### BBC Dataset Summary:\n",
        "- **Total articles:** 2,225\n",
        "- **Categories:** business, entertainment, politics, sport, tech\n",
        "- **File format:** Tab-separated CSV with columns: category, filename, title, content"
      ],
      "metadata": {}
    }
  ]
}